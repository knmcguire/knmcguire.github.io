[
  {
    "location": "https://www.bitcraze.io/2024/12/dr-kims-last-fun-friday-project/#chunk-1",
    "title": "Dr. Kim’s last Fun Friday Project | Bitcraze (Part 1)",
    "text": "Hi everyone! I have a bit of news to share… I’ve decided to leave Bitcraze at the end of 2024. But not before I share with you my latest Fun Friday project that I’ve tried my best to finish up before I leave before my Christmas holiday in December. During the ROSCon talk about the lighthouse system (see therecording here), I’ve already shown a small example of how the lighthouse system could be used on other robots as well. Here you see aPololu RPI 2040(the hyper edition of course), with a slimmed downCrazyflie Boltand aLighthouse deck. The UART2 port on the Bolt (pinoutis the same as Crazyflie) is interfacing with the UART0 connection on the Pololu (pinout). Then the Pololu’s 3v3 is connected to the vUSB and GND to GND (obviously), so 4 wires in total. Technically, the 3v3 port is not supplying enough power for the Crazyflie on paper, but it seemed to be enough as long as the Crazyflie Bolt doesn’t have motors connected it should be fine. But if anyone would like to do a driving-flying hybrid with this combo, you might need to check the specifications a bit closer. For now, just ignore the red low-battery"
  },
  {
    "location": "https://www.bitcraze.io/2024/12/dr-kims-last-fun-friday-project/#chunk-2",
    "title": "Dr. Kim’s last Fun Friday Project | Bitcraze (Part 2)",
    "text": "LED on the Bolt, but if you see it restarting then perhaps give the Pololu a fresh set of batteries. Since the Pololu RPI 2040 doesn’t have any wireless communication, this can be done through the Crazyflie Bolt and the Crazyradio. I’ve made an app layer variant for the Bolt to forward state estimates and velocity commands; however, it did require a bit of an extra logging variable in the firmware itself. But this allows me to control the Pololu through the CFclient! Since it’s using velocity commands, this means that the mobile app is out though, but perhaps if anyone is interested in getting this rolling, let me know. Also, the screen shows the current X, Y, Z, and yaw estimate of the Bolt transferred to the Pololu with the commands that I’ve given it. I’d like to have connected this to a differential drive controller to make use of the position setpoints, but unfortunately the AA batteries ran out at the office and I was unable to complete this by the last day. It would have been great to use the Lighthouse positioning for this. Perhaps in the next coming months, I can try to continue with it"
  },
  {
    "location": "https://www.bitcraze.io/2024/12/dr-kims-last-fun-friday-project/#chunk-3",
    "title": "Dr. Kim’s last Fun Friday Project | Bitcraze (Part 3)",
    "text": "and have my cats chase an autonomous robot around the house, who knows! If anyone is interested in playing around with this, these are the repositories/branches for both the Bolt and the Pololu: First of all, I’ll take a long holiday in the US, first visiting New York (first time) before I hop over to Tulsa and Santa Barbara to visit family. Early 2025 I’ll be taking a long break, or a mini sabbatical of sorts, where I plan to work on some personal projects but mostly have a breather. I haven’t had a break like this in over 15 years, and given a tough 2023, I can definitely say that I’ve deserved some time off. What will happen after, I will hopefully figure out then, but for sure I will be continuing to co-lead theAerial Robotics Interest Groupat ROS and helping out in support of theCrazyswarm2 project. I’d like to thank my colleagues at Bitcraze for an amazing 5 years here in Malmö, Sweden, and everyone that I was able to meet through them. I’ve learned a lot in terms of joint software development, code maintenance, community interaction, and, most importantly, having fun during work. I also will never"
  },
  {
    "location": "https://www.bitcraze.io/2024/12/dr-kims-last-fun-friday-project/#chunk-4",
    "title": "Dr. Kim’s last Fun Friday Project | Bitcraze (Part 4)",
    "text": "forget the support I received while I was going through cancer treatment, and for that I’m very grateful. I wish you all the best and I hope the Crazyflie continues to thrive, saving more PhD projects as it did mine. Thank you."
  },
  {
    "location": "https://www.bitcraze.io/2024/09/crazyflies-adventures-with-ros-2-and-gazebo/#chunk-1",
    "title": "Crazyflie’s Adventures with ROS 2 and Gazebo | Bitcraze (Part 1)",
    "text": "You might remember that at the beginning of this summer, we were invited to do a skill-learning session with the Crazyflie at theRobotics Developer Day 2024(see thisblog post) organized byThe Construct. We showed theCrazyflieflying with themulti-ranger deck, capable of mapping the room in both simulation and the real world. Moreover, we demonstrated this with both manual control and autonomous wall-following. Since then, we wanted to make some improvements to the simulation. We now present an updated tutorial on how to do all of this yourself on your own machine. This tutorial will focus on using the multi-ranger ROS 2 nodes for both mapping and wall-following in simulation first, before trying it out on the real thing. You will be able to tune settings to your specific environment in simulation first and then use exactly the same nodes in the real world. That is one of the main strengths of ROS, providing you with that flexibility. We have made a video of what to expect of the tutorial, for which you should use this blogpost for the more detailed instructions. You’ll need to setup some things first on the PC and acquire hardware to follow this tutorial in full: PC preparation"
  },
  {
    "location": "https://www.bitcraze.io/2024/09/crazyflies-adventures-with-ros-2-and-gazebo/#chunk-2",
    "title": "Crazyflie’s Adventures with ROS 2 and Gazebo | Bitcraze (Part 2)",
    "text": "You’ll need to installROS 2andGazebo simulatormaintained by theOpen Robotics foundationon an Ubuntu machine. Hardware You’ll need to components at least of the STEM ranging bundle If you have any different setup of your computer or positioning system, it is okay as the demos should be simple enough to work, but, be prepared for some warning/error handling that this tutorial might have not covered. Time to complete: This is an approximation of how much time you need to complete this tutorial, depended on your skill level, but if you already have experience with both ROS 2/Gazebo and the Crazyflie it should take1 hour. If you have the Crazyflie for the first time, it would probably be a good idea to go throughthe getting started tutorialand connect to it with aCFclientwith the Flowdeck and Multi-ranger deck attached as a sanity check if everything is working before jumping into ROS 2 and Gazebo. Some things holds for ROS 2! It would be handy to go through theROS 2 Humble beginner tutorialsbefore starting. This section will install 4 packages: Make the workspaces for both simulation and ROS. You can use a different directory for this Let’s clone the repositories in their right location, starting"
  },
  {
    "location": "https://www.bitcraze.io/2024/09/crazyflies-adventures-with-ros-2-and-gazebo/#chunk-3",
    "title": "Crazyflie’s Adventures with ROS 2 and Gazebo | Bitcraze (Part 3)",
    "text": "with simulation Then navigate to the ROS2 workspace source folder and clone 3 projects: First install certain requirements as apt-get packages and pip libraries (might want to make a python environment for the latter) Also follow the instructions to give the proper rights to the Crazyradio 2.0 inthis guide, but if this is your first time of working with theCrazyradio 2.0 first follow this tutorial. Go to the ros2_ws workspace and build the packages Building will take a few minutes. Especially Crazyswarm2 will show a lot of warnings and std_err, but unless the package build has ‘failed’, just ignore it for now until we have proposed a fix to that repository. If the build of all the packages passes and non failed, please continue to the next step! This section will explain how to create a simple 2D map of your environment using the multi-ranger. The ROS 2 package designed for this is specifically made for the multi-ranger, but it should be compatible with NAV2 if you’d like. However, for now, we’ll focus on a simple version without any localization inferred from the map. Open up a terminal which needs to be sourced for both the gazebo model and the"
  },
  {
    "location": "https://www.bitcraze.io/2024/09/crazyflies-adventures-with-ros-2-and-gazebo/#chunk-4",
    "title": "Crazyflie’s Adventures with ROS 2 and Gazebo | Bitcraze (Part 4)",
    "text": "newly build ROS 2 packages: First lets be safe and start with simulation. Startup the ROS 2 launch files with: If you get a ‘No such file or directory’ error on the model,try entering the full path in GZ_SIM_RESOURCE_PATH export. Gazebo will start with the Crazyflie in the center. You can get a close-up of the Crazyflie by right-clicking it in the Entity tree and pressing ‘Move to’. You can also choose to follow it, but the camera tracking feature of Gazebo needs some tuning to track something as small as the Crazyflie. Additionally, you will see RVIZ starting with the map view and transforms preconfigured. Open up another terminal, source the installed ROS 2 distro and open up the ROS 2 teleop keyboard node: Have the Crazyflie take off with ‘t’ on your keyboard, and rotate it around with the teleop instructions. In RVIZ you should see the map being created and the transform of the Crazyflie moving. You should be able to see this picture, and inthis part of the video. Now that you got the gist of it, let’s move to the real Crazyflie! First, if you have a different URI of the Crazyflie to connect to,"
  },
  {
    "location": "https://www.bitcraze.io/2024/09/crazyflies-adventures-with-ros-2-and-gazebo/#chunk-5",
    "title": "Crazyflie’s Adventures with ROS 2 and Gazebo | Bitcraze (Part 5)",
    "text": "first change the config file ‘crazyflie_real_crazyswarm2.yaml’ in the crazyflie_ros2_repository. This is a file that Crazyswarm2 uses to know to which Crazyflie to connect to. Open up the config file in gedit or your favorite IDE like visual code: and change the URI onthis line specificallyto the URI of your Crazyflie if necessary. Mind that you need torebuild ros2_ws againto make sure that this has an effect. Now source the terminal with the installed ROS 2 packages and the Gazebo model, and launch the ROS launch of the simple mapper example for the real world Crazyflie. Now open up another terminal, source ROS 2 and open up teleop: Same thing, have the Crazyflie take off with ‘t’, and control it with the instructions. You should be able to see this on your screen, which you can also check withthis part of the video. Make the Crazyflie land again with ‘b’, and now you can close the ROS 2 node in the launch terminal with ctrl + c. Previously, you needed to control the Crazyflie yourself to create the map, but what if you could let the Crazyflie do it on its own? The `crazyflie_ros2_multiranger` package includes a `crazyflie_ros2_multiranger_wall_following` node that uses"
  },
  {
    "location": "https://www.bitcraze.io/2024/09/crazyflies-adventures-with-ros-2-and-gazebo/#chunk-6",
    "title": "Crazyflie’s Adventures with ROS 2 and Gazebo | Bitcraze (Part 6)",
    "text": "laser ranges from the multi-ranger to perform autonomous wall-following. Then, you can just sit back and relax while the map is created for you! Let’s first try it in simulation, so open up a terminal and source it if you haven’t already (see section of the Simple mapper simulation). Then launch the wall follower ROS 2 launch file: Take off and wall following will go fully automatic. The simulated Crazyflie in Gazebo will fly forward, stop when it sees a wall with it’s forward range sensor and follow the wall on its left-hand side. You’ll see on RVIZ2 when the full map is created like here below and this part of thetutorial video. You can stop the simulated Crazyflie by the following service call in another terminal that is sourced with ROS 2 humble. The simulated Crazyflie will stop wall following and land. You can also just close the simulation, since nothing can happen here. Now that we have demonstrated that the wall-following works in simulation, we feel confident enough to try it in the real world this time! Make sure you have a fully charged battery, place the Crazyflie on the floor facing the direction you’d like the positive"
  },
  {
    "location": "https://www.bitcraze.io/2024/09/crazyflies-adventures-with-ros-2-and-gazebo/#chunk-7",
    "title": "Crazyflie’s Adventures with ROS 2 and Gazebo | Bitcraze (Part 7)",
    "text": "x-axis to be (which is also where it will fly first), and turn it on. Make sure that you are flying with a room with clear defined walls and corners, or make something with cardboard such as a mini maze, but the current algorithm is optimized to just fly in a squarish room. Source the ROS 2 workspace like previously and start up the wall follower launch file for the Like the simulated Crazyflie, the real Crazyflie will take off automatically and automatically do wall following, so it is important that it is flying towards a wall. It should look like this screenshot, or you can check it withthis part of the video. Be careful here to not accidently run this script with the Crazyflie sitting on your desk! If you’d like the Crazyflie to stop,don’t stop theROS2 nodes with ctrl-c, since it will continue flying until crash. It’s not like simulation unfortunately where you can close the environment and nothing will happen.Instead, use the ROS 2 service made for this in a different terminal: Similar the real Crazyflie will stop wall following and land. Now you can close the ROS 2 terminals and turn off the crazyflie. We don’t"
  },
  {
    "location": "https://www.bitcraze.io/2024/09/crazyflies-adventures-with-ros-2-and-gazebo/#chunk-8",
    "title": "Crazyflie’s Adventures with ROS 2 and Gazebo | Bitcraze (Part 8)",
    "text": "have any more demos to show but we can give you a list of suggestions of what you could try next! You could for instance have multiple Crazyflies mapping together like in the video shown here: This uses themapMergeForMultiRobotMapping-ROS2external project, which is combined withCrazyswarm2withthis launch file gist. Just keep in mind that, currently, it would be better to use a global positioning system here, such as the Lighthouse positioning system used in the video. Also, if you’d like to try this out in simulation, you’ll need to ensure different namespaces for the Crazyflies, which the current simulation setup may not fully support. Another idea is to connect the NAV2 stack instead of the simple mapper. There exists a couple of instructions onthe Crazyswarm2 ROS2 tutorialsso you can use those as reference. Check out the video below here. Moreover, if you are having difficulties setting up your computer, I’d like to remind you that the skill-learning session we conducted forRobotics Developer Daywas entirely done using a ROSject provided byThe Construct, which also allows direct connection with the Crazyflie. The only requirement is that you can run Crazyswarm2 on your local machine, but that should be feasible. See the video of the"
  },
  {
    "location": "https://www.bitcraze.io/2024/09/crazyflies-adventures-with-ros-2-and-gazebo/#chunk-9",
    "title": "Crazyflie’s Adventures with ROS 2 and Gazebo | Bitcraze (Part 9)",
    "text": "original Robotics Developer Day skill-learning session here: The last thing to know is that the ROS 2 nodes in this tutorial are running ‘offboard,’ so not on the Crazyflies themselves. However, do check out theMicro-ROS examples for the CrazyfliebyEprosimawhenever you have the time and would like to challenge yourself with embedded development. That’s it, folks! If you are running into any issues with this tutorial or want to bounce some cool ideas to try yourself, start a discussion thread onhttps://discussions.bitcraze.io/. Happy hacking!"
  },
  {
    "location": "https://www.bitcraze.io/2022/10/crazyswarm2-development/#chunk-1",
    "title": "Crazyswarm2 development | Bitcraze (Part 1)",
    "text": "As you probably noticed already, this summer I experimented with ROS2 and connecting the Crazyflie with multi-ranger to several mapping and navigation nodes (seethisandthis blogpost). First I started with an experimental repo on my personal Github account calledcrazyflie_ros2_experimental, where I managed to do some mapping and navigation already. In August we started porting most of this functionality tothe crazyswarm2 project, so that is what this blogpost is mostly about. Most of you are already familiar withCrazyswarmfor ROS1, which is a project that Wolfgang Hönig and James Preiss have maintained since its creation in 2017 atthe University of Southern California. Since then, many have used and referred to this work, sincethe paperhas been cited more than 260 times. From all the Crazyflie papers of the latestICRAandIROSconferences, 50 % of the papers have used Crazyswarm as their communication middleware. If you haven’t heard about Crazyswarm yet, please check-out the niceBAMdays talkWolfgang gave last year. Unfortunately, ROS1 will not be there forever and will be phased out anno 2025 and will not be supported for Ubuntu 22.04 and up. Therefore, Wolfgang, now at theIntelligent Multi-robot Coordination Lab at TU Berlin, has already started with the ROS2 port of Crazyswarm, namelyCrazyswarm2. Here the same"
  },
  {
    "location": "https://www.bitcraze.io/2022/10/crazyswarm2-development/#chunk-2",
    "title": "Crazyswarm2 development | Bitcraze (Part 2)",
    "text": "principle of the C++ based Crazyflie server and the python wrapper were been implemented, along with the simple position based simulation and Teleop nodes. Mind that the name Crazyswarm2 is just the project name out of historic reasons, but the package itself can also be used for individual Crazyflies as well. That is why the package names will be called crazyflie_* Thecrazyflie_ros2_experimentalwas fun to hack around, as it was (as the name suggests) experimental and I didn’t need to worry about releases, bugfixes etc. However, the problem of developing only here, is that the further you go the more work it becomes to make it more official. That is when Wolfgang and I sat down and started talking about porting what I’ve done in the summer into Crazyswarm2. This is also a good opportunity to get more involved with the project, especially with so many Crazyfliers using the ROS as well. The first step was to write a second crazyflie_server node that relied on thepython CFlib. This means that many of the variables I used to hardcode in the experimental node, needed to be defined within the parameter structure of ROS2. Thecrazyflies.yamlis where anything relevant for the server (like the"
  },
  {
    "location": "https://www.bitcraze.io/2022/10/crazyswarm2-development/#chunk-3",
    "title": "Crazyswarm2 development | Bitcraze (Part 3)",
    "text": "URIs and parameters) needs to be defined. Both the C++ backend server and the CFlib backend server are using the same parameters. Also the functionality of the both servers are pretty similar, except for that logging is only possible on the CFlib version and uploading/follow trajectories is only possible on the C++ version. An overview will be provided soon on theCrazyswarm2 documentationwebsite. The second step was to make the crazyflie_server (cflib) node suitable to be connected to external packages that I’ve worked with during the hack project. Therefore, there are some special logging modes, that enables the server to not only output topics based on logging, butPose/Odometry/LaserScanmessages along withTransforms. This allowed the SLAM_toolbox to use the data from the Crazyflie itself to create a map, which you can see an example of inthis tutorial. Moreover, for the navigation it was important that incomingTwistmessages either fromkeyboardor froma navigation toolkitwere handled properly. Most of these packages assume a 2D non-holonomic robot, but a quadcopter like the Crazyflie needs to first take off, stay in the air and land. Therefore in the examples, a separate node (vel_mux.py) was written to receive incoming Twist messages, first have the Crazyflie take off in high level"
  },
  {
    "location": "https://www.bitcraze.io/2022/10/crazyswarm2-development/#chunk-4",
    "title": "Crazyswarm2 development | Bitcraze (Part 4)",
    "text": "commander, and keep sending hover commands to keep it in the air until a land service is called. As you probably noticed, the project is still under development, but at least it is now at a good state that we feel comfortable to presentedat the upcoming ROScon:) We also want to include an more official simulation package, especially now that theCrazyfliehas recently became part of the official release ofWebots 2022b, but we are currently waiting on thewebots_ros2to be released in the ubuntu packages. Moreover, the idea is to provide multiple simulation backends that based on the requirement of the topic (swarms, vision-based etc), the user can select the simulation most useful for their situation. Also, we would like to even out the missing items (trajectory handling, logging) in both the cflib and cpp backend of the crazyflie_server so that they can be used interchangeably. Also, I saw that the experimentalsimple mapper node has been featured on social media, so perhaps we should be converting that to Crazyswarm2 as well :) So once we got the most of the above mentioned issues out the way, that will be the time that we can start discussing the official release of a ROS2"
  },
  {
    "location": "https://www.bitcraze.io/2022/10/crazyswarm2-development/#chunk-5",
    "title": "Crazyswarm2 development | Bitcraze (Part 5)",
    "text": "Crazyflie package with its source code residing in the Crazyswarm2 repository. In the meantime, it would be awesome that anybody that is interested in ROS2, or want to soon upgrade their Crazyswarm(1) packages to ROS2 to give the package a whirl. The more people that are trying it out and report bugs/proposing fixes, the more stable it becomes and closer it will come to an official release! Please join us and start any discussions on theCrazyswarm2 project github repository."
  },
  {
    "location": "https://www.bitcraze.io/2022/08/crazyflie-ros2-summer-update/#chunk-1",
    "title": "Crazyflie ROS2 summer update | Bitcraze (Part 1)",
    "text": "Now it is time to give a little update about the ongoingROS2related projects. About a month ago we gave you an heads-up about theSummer ROS2 projectI was working on, and even though the end goal hasn’t been reached yet, enough has happened in the mean time to write a blogpost about it! Last timeshowed mostly mapping of a single room, so currently I’m trying to map a bigger portion of the office. This was initially more difficult then initially anticipated, since it worked quite well in simulation, but in real life themulti-ranger decksaw obstacles that weren’t there. Later we found out that was due tothis year old issueof the multi-ranger’s driver incapability to handle out-of-range measurements properly (seethis ongoing PR). With that, larger scale mapping starts to become possible, which you can see here with the simple mapper node: If you look at the video until the end, you can notice that the map starts to diverge a bit since the position + orientation is solely based on theflow deckand gyroscopes , which is a big reason to get theSLAM toolboxto work with the multi-ranger. However, it is difficult to combine it with such a sparse ‘Lidar’ , so while"
  },
  {
    "location": "https://www.bitcraze.io/2022/08/crazyflie-ros2-summer-update/#chunk-2",
    "title": "Crazyflie ROS2 summer update | Bitcraze (Part 2)",
    "text": "that still requires some tuning, I’ve taken this opportunity to see how far I get with the non-slam mapping andthe NAV2 package! As you see from the video, the Crazyflie until the second hallway. Afterwards it was commanded to fly back based on a NAV2 waypoint in RViz2. In the beginning it seemed to do quite well, but around the door of the last room, the Crazyflie got into a bit of trouble. The doorway entrance is already as small as it is, and around that moment is also when the mapping started to diverge, the new map covered the old map, blocking the original pathway back into the room. But still, it came pretty close! The diverging of the map is currently the blocker for larger office navigation, so it would be nice to get some better localization to work so that the map is not constantly changed due to the divergence of position estimates, but I’m pretty hopeful I’ll be able to figure that out in the next few weeks. Based on thepollwe set out in the last blogpost, it seemed that many of you were mostly positive for work towards a ROS2 node for the Crazyflie! As"
  },
  {
    "location": "https://www.bitcraze.io/2022/08/crazyflie-ros2-summer-update/#chunk-3",
    "title": "Crazyflie ROS2 summer update | Bitcraze (Part 3)",
    "text": "some of you know, theCrazyswarm project,that many of you already use for your research, is currently being ported to ROS2 with efforts ofWolfgang Hönig’s IMRCLabwith theCrazyswarm2 project. Instead of in parallel creating separate ROS2 nodes and just to add to the confusion for the community, we have decided with Wolfgang to place all of the ROS2 related development into Crazyswarm2. The name of the project will be the same out of historical reasons, but since this is meant to be the standard Crazyflie ROS2 package,the names of each nodes will be more genericupon official release in the future. To this end, we’ve pushed acflib pythonversion of thecrazyflie ros2 nodecalled crazyflie_server_py, a bit based on my hackish efforts of thecrazyflie_ros2_experimentalversion, such that the users will have a choice of which communication backend to use for the Crazyflie. For now the node simply creates services for each individual Crazyflie and the entire swarm fortake_off,landandgo_tocommands. Next up are logging and parameter handling, positioning support and broadcasting implementation for the CFlib, so please keep an eye onthis ticketto see the process. So hopefully, once the summer project has been completed, I can start porting the navigation capabilities into the the Crazyswarm2 repository with a"
  },
  {
    "location": "https://www.bitcraze.io/2022/08/crazyflie-ros2-summer-update/#chunk-4",
    "title": "Crazyflie ROS2 summer update | Bitcraze (Part 4)",
    "text": "nice tutorial :) As mentionedin a previous blogpost, we’ll actually be talking about the Crazyflie ROS2 efforts at ROScon 2022 in Kyoto in collaboration with Wolfgang. You can findthe talk here in the ROScon program, so hopefully I’ll see you at the talk or the week after at IROS!"
  },
  {
    "location": "https://www.bitcraze.io/2022/07/crazyflie-summer-project-with-ros2-and-mapping/#chunk-1",
    "title": "Crazyflie Summer Project with ROS2 and Mapping | Bitcraze (Part 1)",
    "text": "In the first years that I started at Bitcraze I’ve been focused mostly on embedded development and algorithmic design like the app layer, controllers and estimators and such, however recently I started to be quite interested in the robotic integration between the Crazyflie and other (open-source) projects and users. This means that I’ll be dwelling more often in the space between Bitcraze and the community, which is something that I do really enjoy I noticed duringthe Grand Tour. It also initiated mywork with simulatorswhich I think would be very useful for the community too. The summer fun project that I’ve been now working on is to integrate the Crazyflie with ROS2 to integrate standard navigational packages, which will be the topic of this blogpost! So first I worked on the ROS2 node that actually communicates with the Crazyflie directly. I think many of you are familiar with the USC’sCrazySwarm project, of which the ROS2 variant,CrazySwarm2,is already available for most functionalities. Even though the name says CrazySwarm, this can be very easily used for only one Crazyflie too. The CrazySwam2 is currently under more development by theIMRClabof TU Berlin, but please take a look if you want to give it a"
  },
  {
    "location": "https://www.bitcraze.io/2022/07/crazyflie-summer-project-with-ros2-and-mapping/#chunk-2",
    "title": "Crazyflie Summer Project with ROS2 and Mapping | Bitcraze (Part 2)",
    "text": "go! For now while Crazyswarm2 is still under development, I used the BitcrazeCrazyflie python libraryto make a more hackish node that just publishes exactly the information I want. I am focusing on the scenario with theSTEM rangingbundle, aka the Crazyflie +Flowdeck(optical flow + distance sensor) +Multi-ranger(5 x distance sensors) combo, where the node logs the multi-ranger data and the odometry from the Flowdeck with theCrazyradioand outputs that into necessary /scan and /odom topics. Moreover, it also outputs several tf2 transforms that makes it possible to either visualize it in RVIZ and/or connect it to any other packages and it should react to incoming twist messages as well. And of course… I went in head first and connected it directly with theSLAM toolbox. I have worked with ROS1 in the past, but I had my first experience working with that package in the course:Build Mobile Robots with ROS2(by Weekly Robotic Newsletter’s Mat Sadowski), so I couldn’t wait to try it on a real platform like the Crazyflie. However, tuning this was of course more work than I thought, as the map that I got out of it first was mostly a sparse collection of dots. Of course the SLAM toolbox is"
  },
  {
    "location": "https://www.bitcraze.io/2022/07/crazyflie-summer-project-with-ros2-and-mapping/#chunk-3",
    "title": "Crazyflie Summer Project with ROS2 and Mapping | Bitcraze (Part 3)",
    "text": "meant for lidars and not something that provided sparse range distances like the Multiranger. Then I decided to take one or two steps back, and first connect a simulator to make tuning a bit easier. Luckily, I’ve already started to look at simulators, and was quite far in the Webots integration of the Crazyflie. Actually… Webots’ next release (2022b) will contain a Crazyflie as standard! Once it is out, I’ll write a blogpost about that separately :). As luck has it, Webots also has goodROS2 integrationas well, and even won the ‘Best ROS Software’ award byThe Construct’s ROS awards! Another reason is that I wanted to try out a different simulator for ROS2 this time to complement what I’ve learned in the ROS2 course I mentioned earlier. So I used the webots driver node to write a simulated Crazyflie that should output the same information as the real Crazyflie node, so that I can easily hack around and try out different things without constantly disturbing my cats from their slumber :). Anyway, I won’t go into to the simulator too much and save that for another blogpost! I decided to also take another additional step before going full SLAM, which"
  },
  {
    "location": "https://www.bitcraze.io/2022/07/crazyflie-summer-project-with-ros2-and-mapping/#chunk-4",
    "title": "Crazyflie Summer Project with ROS2 and Mapping | Bitcraze (Part 4)",
    "text": "is to make a simple mapper node first! This takes the estimated state estimate of the Crazyflie and the Multiranger’s range values and it creates an occupancy grid type map of it. I do have to give kudos to theMarcus’ cflib Pointcloud scriptand Webot’s simple mapper example, as I did look at them for some reference. But still with the examples, integration and connecting the dots together is quite some work. Luckily I had the simulator to try things out with! So first I put the Crazyflie in an apartment simulator, flew around and see if any decent maps comes out of it and it seemed it did! Of course, the simulated Crazyflie’s ‘odometry’ comes from near perfect position estimate, so I didn’t expect any problems there (and in such a situation you would actually not really need the localization part of SLAM). This still needs some improvements to be done, like now range measurements that don’t see anything are excluded from drawing, but still it was pretty cool tomap the virtual environment. So it was off to try it out on a real crazyflie. In one of our meeting rooms, I had one Crazyflie take off, let it turn"
  },
  {
    "location": "https://www.bitcraze.io/2022/07/crazyflie-summer-project-with-ros2-and-mapping/#chunk-5",
    "title": "Crazyflie Summer Project with ROS2 and Mapping | Bitcraze (Part 5)",
    "text": "around with a twist message in a /cmd_vel topic andmade a map of the roomI was currently in. The effect of the 4 range sensors rotating around and creating a map in one go, makes me think of these retro video transitions. And the odometry drift does not seem as bad for it to be possible, but I haven’t mapped our entire office yet so that might be different! So I’m not stopping here for sure, I want to extend this functionality further and for sure get it to work with the SLAM_toolbox properly! But if the simple mapper already can produce such quality, I’m pretty sure that this can be done in one way or the other. What I could also do, is first generate a simple map and already have a go atthe NAV2 packagewith that one… there are many roads to Rome here! Currently I’m doing my work on my personal Github account in thecrazyflie_ros2_experimentalrepository. Everything is stillvery muchin development, hackish and quite specific for one use case but that is expected to change once things are working better, so please check the planning in the project’s readme. In the mean time, you can indicate to us"
  },
  {
    "location": "https://www.bitcraze.io/2022/07/crazyflie-summer-project-with-ros2-and-mapping/#chunk-6",
    "title": "Crazyflie Summer Project with ROS2 and Mapping | Bitcraze (Part 6)",
    "text": "in this vote if this isan interesting directionfor us to go towards. Not that it will stop me from continuing this project since it is too much fun, but it is always good to know if certain efforts are appreciated!"
  },
  {
    "location": "https://www.bitcraze.io/2022/03/updates-on-simulation-work/#chunk-1",
    "title": "Updates on Simulation work | Bitcraze (Part 1)",
    "text": "In December we had ablogpostwhere we gave an overview of existing simulation models that were out there. In the mean time, I have done some work during myFun Fridaysto get this to work even further. Currently I moved the efforts from my personal Github repo to the Bitcraze organization github calledcrazyflie-simulation. It is all still very much work in progress but in this blogpost I will explain the content of the repository and what these elements can already do. The first thing that you will need to have for any simulation, is a 3D model of the Crazyflie. There is of course already great models available from theCrazyS project, thesim_cfproject and themulti_uav_simulator, which are completely fine to use as well. But since we have direct access to the exact geometries of the real crazyflie itself, I wanted to see if I could abstract the shapes myself. And also I would like to improve myBlenderskills, so this seemed to be a nice project to work with! Moreover, it might be handy to have a central place if anybody is looking for a 3D simulation model of the Crazyflie. For simulations with only one or a few Crazyflie, the higher resolution models"
  },
  {
    "location": "https://www.bitcraze.io/2022/03/updates-on-simulation-work/#chunk-2",
    "title": "Updates on Simulation work | Bitcraze (Part 2)",
    "text": "from the other repository are absolutely sufficient, especially if you are not using a very complicated physics geometry model (because that is where most of the computation is). But if you would like to simulate very big swarms, then the polygon count will have more influences on the speed of the simulation. So I managed to make it to 1970 vertices with the below Crazyflie model, which is not too bad! I am sure that we can make it even with lesser polygons but this is perhaps a good place to start out with for now. In thecrazyflie-simulation, you can find the Blender, stl files and collada files under the folder ‘meshes’. We implemented the above model in aWebots simulator, which was much easier to implement than I thought!The tutorialsthey provide are great so I was able to get the model flying within a day or two. By combining thepropellernode androtational motor, and adjusting the thrust and drag coefficient to be a bit more ‘Crazyflie like’, it was able to take off. It would be nice to perhaps base these coefficients on the system identification of the Crazyflie, like what was done for thisbachelor thesis, but for now our goal"
  },
  {
    "location": "https://www.bitcraze.io/2022/03/updates-on-simulation-work/#chunk-3",
    "title": "Updates on Simulation work | Bitcraze (Part 3)",
    "text": "is just to make it fly! The webots model can found in thesame simulation repositoryunder/webots/. You can try out the model by It would then be possible to control the pitch and roll with the arrow keys of your keyboard while it is maintaining a current height of 1 meter. This is current state of the code as of commit79640a. Ignitionwill be the replacement for Gazebo Classic, which is already a well known simulator for many of you. Writing controllers and plugins is slightly more challenging as it is only in C++ but it is such a landmark in the world of simulation, it only makes sense that we will try to make a Gazebo model of the Crazyflie as well! In theprevious blogpostI mentioned that I already experimented a bit with Ignition Gazebo, as it has the nicemulticopter motor modelplugin standard within the framework now. Then I tried to make it controllable with the intergrated multicopter velocity control pluginbut I wasn’t super successful, probably because I didn’t have the right coefficients and gains! I will rekindle these efforts another time, but if anybody would like to try that out, please do so! First I made my own controller plugin"
  },
  {
    "location": "https://www.bitcraze.io/2022/03/updates-on-simulation-work/#chunk-4",
    "title": "Updates on Simulation work | Bitcraze (Part 4)",
    "text": "for the gazebo model, which can be found in the repository in a different branchunder /gazebo-ignition/. This controller plugin needs to be built first and it’s bin file added to the path IGN_GAZEBO_SYSTEM_PLUGIN_PATH, and the Crazyflie model in IGN_GAZEBO_RESOURCE_PATH , but then if you try to fly the model with the following: It will take off and hover nicely. Unfortunately, if you try out the key publisher widget with the arrow keys, you see that the Crazyflie immediately crashes. So there is still something fishy there! Please check out the issue list of the repo to check the state on that. So the reason why I made my own controller plugins for the above mentioned simulation models, is that I want to experiment with a way that we can separate thecrazyflie firmware controllers, make a code wrapper for them, and use those controllers directly in the simulator. So this way it will become a hybrid software in the loop without having to compile the entire firmware that contains all kinds of extra things that the simulation probably does not need. We can’t do this hybrid SITL yet, but at least it would be nice to have the elements in place"
  },
  {
    "location": "https://www.bitcraze.io/2022/03/updates-on-simulation-work/#chunk-5",
    "title": "Updates on Simulation work | Bitcraze (Part 5)",
    "text": "to make it possible. Currently I’m only experimenting with a simple fixed height and attitude PID controller written in C, and some extra files to make it possible to make a python wrapper for those. The C-controller itself you can try out in Webots as of this commit79640a, but hopefully we will have the python version of it working too. As you probably noticed, the simulation work are still very much work in process and there is still a lot enhancements to add or fix. Currently this is only done on available Fridays so the progress is not super fast unfortunately, but at least there is one model flying. Some other elements that we would like to work on: I might turn a couple of these into topics that would be good for contribution, so that any community members can help out with. Please keep an eye onthe issue list, and we are communicating on theCrazyswarm2 Discussion pageabout simulations if you want to share your thoughts on this as well."
  },
  {
    "location": "https://www.bitcraze.io/2021/12/simulation-possibilities/#chunk-1",
    "title": "The State of Crazyflie Simulations | Bitcraze (Part 1)",
    "text": "I have returned from my family visit in California, who I’ve haven’t seen them in 3 years due to Covid. To spend the most possible time with them, the plan was that I would still work full time for Bitcraze from my father’s home. The problem became however, that it wouldn’t fit so well in our currentway of workas I would miss all the morning stand up meetings due to the large time difference between Sweden and California (-9 hours). That is why we settled that I would work on separate projects/investigations during my time away. So I thought it would be a great opportunity to dig intoROSand 3D simulations again and see what the latest state of that is! So about the simulations is what I’ll be mostly talking about right in this blog post, in terms of what simulators are out there and what simulation development is currently ongoing. Why would it be actually be necessary to have a simulation in our current frame work? Just to give an example, my new colleague Jonas recently tried out his hand on the CFlib swarm class for the first time for theBAMdaystutorials, and simulator would have been great during that"
  },
  {
    "location": "https://www.bitcraze.io/2021/12/simulation-possibilities/#chunk-2",
    "title": "The State of Crazyflie Simulations | Bitcraze (Part 2)",
    "text": "initial porcess. Namely, most of the crashes were not necessary due to low batteries or bad communication, but mostly due to the fact that he was not able to double check his script beforehand. If one is able to check if all the programmed positions of the Crazyflies are implemented as they should before an actual flight, this would prevented a lot of broken propellers! Just to note here that there are a lot of types of simulations that you can think of. Earlier this year had our ex-interns Max and Josephine finish anRenode simulation of the Crazyflie’s microcontrollers. We’ve also seen the word Simulink pop-up multiple times on theforumwhich indicates that quite some control classes are investigating the dynamic model of the Crazyflie. However, the type of simulation that I’m currently referring to are the 3D simulators in which a robot or quadcopter can move and interact with a virtual environment, with usually an physics engine in effect. During some initial investigation there were already some simulations that pop out. First of all I went and looked into what is available forGazeboat the moment, which is: CrazyS is based on theRotorS simulationwith some additional off-board crazyflie controllers for position"
  },
  {
    "location": "https://www.bitcraze.io/2021/12/simulation-possibilities/#chunk-3",
    "title": "The State of Crazyflie Simulations | Bitcraze (Part 3)",
    "text": "control. I wasn’t able to build it for my Ubuntu 20.04 just yet myself, but that there isongoing workto port CrazyS to ROS Noetic. For now on a virtual machine with ROS melodic it build just fine! Note my laptop did had to work quite hard when I wanted to simulate more than 1 Crazyflie, but the physics and plugins that were made for Gazebo is enabling many to do a lot for their research. Please check outthe core papersabout CrazyS! Sim_cf is perhaps a little lesser known, but the project does stand out as it has some interesting features to it. It is for instance, possible to use theactual c-based firmwarein software-in-the-loop (SITL) mode, which controls the simulated Crazyflie. It is even possible to use an actual crazyflie with an hardware-in-the-loop (HITL) simulation. Eventhough the project is not actively maintained anymore, I did manage to build it from source for ROS Noetic and Gazebo 11, although I was not able to fly more than 4 do to errors. Ofcourse Gazebo is not the only possibility out there. I also had a quick go at another simulator calledWebots, which is quite an interesting option indeed as well. Currently there is"
  },
  {
    "location": "https://www.bitcraze.io/2021/12/simulation-possibilities/#chunk-4",
    "title": "The State of Crazyflie Simulations | Bitcraze (Part 4)",
    "text": "only one quadcopter model available, so it only makes sense for it to also contain an Crazyflie! They do use their own robotic format, so probably the easiest process would be, is to convert an existing model for Gazebo/ROS into an format that Webots can understand. Also, quite recently,a trending tweethas brought us to the attention of aRviz based Crazyflie simulation! This looks quite promising as well, so I will try this out quite soon too. So in the future, the currentGazeboin its form will disappear and will be only be part ofIgnition. So that is why it made sense for me to start playing with an separate Crazyflie model and plugins for the Ignition frame work instead. Moreover, it seems that quite some elements and plugins based on the RotorS simulation for the original gazebo, are now fully integrated within the Ignition gazebo framework, which should make it more easier to make quadcopter models fly. Currently it’s still work in progress, so right now is only to be found on mypersonal githubrepository, but as soon as it becomes more fleshed out and stable, this will probably transferred to Bitcraze’s github repos and we will write a more elaborate blogpost"
  },
  {
    "location": "https://www.bitcraze.io/2021/12/simulation-possibilities/#chunk-5",
    "title": "The State of Crazyflie Simulations | Bitcraze (Part 5)",
    "text": "about it. For now, I’ll try to work on it further as myFun Friday project! In the mean time, we have started asimulation discussion threadin the Crazyswarm2 repository, which is an ongoing port ofCrazyswarmtoROS2. It would be the ideal situation if we would be able to use this simulator for bothCrazyswarmand our native CFlib! But I’ve mostly have used Gazebo in the past, so if there are any other simulators that we should try out too, please join the discussion and let us know!"
  },
  {
    "location": "https://www.bitcraze.io/2021/09/do-ai-decks-dream-of-tutorials/#chunk-1",
    "title": "Do AI decks dream of tutorials? | Bitcraze (Part 1)",
    "text": "The AI-decks are back in stock! Also, last week we had our quarterly meeting, where we plan our focus for the next quarter. As it is also the start of the fiscal year, we also take this opportunity to update our 1 year and 3 year plans as well. We have a big plans coming up, but one of the important focuses that we will have this year, is to get theAI-deckout of early-access! But what would be necessary for such a task? The AI-deck is one of the most complicated boards we have worked with, so do we have to evaluate its ‘out-of-readiness’ along the same standards than any of our other products. So one of our idea is to be able to achieve a state of the AIdeck in order to write a mega AIdeck tutorial series. This implies that we are able to show how somebody could go from a datasets all the way to a flying aideck-crazyflie combo. Such a series could consist of the following topics: From the first look of it, this sounds like it should be easy to do right? Actually, there are still much to be done in order to make this"
  },
  {
    "location": "https://www.bitcraze.io/2021/09/do-ai-decks-dream-of-tutorials/#chunk-2",
    "title": "Do AI decks dream of tutorials? | Bitcraze (Part 2)",
    "text": "tutorial possible. One of the more challenging aspects of the AIdeck as it now is, is that users need to buy a JTAG-enabled programmer in order to flash the GAP8 and the NINA module of the AI-deck. That is the reason why currently the AIdeck has these 2 x 10 pin jtag connectors attached, but ideally we would want to get rid of it completely. This means is that we need to have over air flashing of the GAP8’s binary and that the intercommunication of the NINA and AIdeck will become even more important. Moreover, the communication protocol from the GAP8 to the STM32 of the Crazyflie is currently very basic, as of right now, it is only possible to send single characters. It might work in some situations, but what if you would like to send an array of values back to the Crazyflie, like the collision probability & steering angle like inPULP platform’s implementation of Dronet? And, would we like to keep on using two UART serial ports or perhaps just relay both NINA and GAP8 communication all through one? The later will make it easier for us to maintain the crazyflie-aideck communication but can perhaps introduce communication"
  },
  {
    "location": "https://www.bitcraze.io/2021/09/do-ai-decks-dream-of-tutorials/#chunk-3",
    "title": "Do AI decks dream of tutorials? | Bitcraze (Part 3)",
    "text": "delays. These are just a slice of the type of re-plumbing work for the AIdeck before we can even start our dream tutorial series, but at least it will give you an idea of what we are dealing with :) From this week we have the honor of hosting Rik Bouwmeester for a couple of months. He is currently doing his Master Thesis at theMAVlabfrom thefaculty of Aerospace Engineering of the TU Delft. Since he has experience of working the AIdeck before, he will be able to provide us with some user perspective and help us with the above mentioned issues. You can expect a blogpost from him soon!"
  },
  {
    "location": "https://www.bitcraze.io/2021/05/starting-development-with-the-ai-deck/#chunk-1",
    "title": "Starting Development with the AI-deck | Bitcraze (Part 1)",
    "text": "Ever since theAI-deck 1.x was released in early access, we’ve been excited to see so many users diving in and experimenting with it. The product is still inearly access, where the hardware is deemed ready but the software and documentation still needs work. Even so, we try to do our best to make the product as usable as possible. We’re happy to see some of our users doing great stuff, like thepulp-platformslatest paper “Fully Onboard AI-powered Human-Drone Pose Estimation on Ultra-low Power Autonomous Flying Nano-UAVs“. The AI-deck consists of theGAP8 chipdeveloped byGreenwaves Technologies. On their website there’s anexplanation of development toolswhere you get a general understanding of what you can use. Also theirGAP SDK documentationexplains how to install and try out some of their examples as well, on both a GAP8 simulator on the computer or on the GAP8 chip on the AI-deck itself. We also host anseparate repository for some AIdeck related exampleswhich runs with the GAP SDK. Recently we also added theAIdeck documentation to the Bitcraze website, generated from the docuemtnation already available in theGithub repository. There’s still improvements to make, so if you find any issues or any additions needed, don’t hesitate to help us improve it."
  },
  {
    "location": "https://www.bitcraze.io/2021/05/starting-development-with-the-ai-deck/#chunk-2",
    "title": "Starting Development with the AI-deck | Bitcraze (Part 2)",
    "text": "On the bottom there is an ‘improve this page’ link where you can give the suggested change, or notify us by posting onthe issue list of the AIdeck repository. Also note that we havea separate AI-deck category on our forumwhere you can search for or add any AI-deck related questions. Remember that posting the issues that you are having will also help us to improve the platform and hopefully soon get it out of Early Access. On the 16th of April we hosted a workshop given byPULP-platformfeaturing Greenwaves Technologies. In the workshop the an overview of the AI-deck and GAP8 was given as well as going through some basic hands-on exercises. About 70 people joined the workshop and we were happy it was so well received. The workshop is a great source of information for anybody who is just getting started with the AI-deck, so have a look at therecordings on Youtubeandthe slides on the event page. Also make sure to check out theirPULP training pagefor more tools that also can be used on the AI-deck. A big thanks to the PULP-platform and Greenwaves Technologies for taking part in the workshop! Also we would like to ask if anybody who"
  },
  {
    "location": "https://www.bitcraze.io/2021/05/starting-development-with-the-ai-deck/#chunk-3",
    "title": "Starting Development with the AI-deck | Bitcraze (Part 3)",
    "text": "joined the workshop,to fill in this small questionnaireso that we can get some more feedback on how it went and how we can improve for the next one."
  },
  {
    "location": "https://www.bitcraze.io/2020/08/flow-and-lps-deck-improvements-in-progress/#chunk-1",
    "title": "Flow- and LPS-deck improvements in progress | Bitcraze (Part 1)",
    "text": "Now that we are all back fromour summer holiday, we are back on what we were set on doing a while ago already: fixing issues and stabilizing code. In the last two weeks we have been focusing on fixing existing issues of the Flowdeck and LPS positioning system. It is still work in progress and even though we fixed some problems, we still have some way to go! At least we can give you an update of our work of the last few weeks. When we started working on themotion commander tutorials(see thisblogpost), which are mostly based on flying with the flowdeck, we were also hit by the following error that probably many of you know: the Crazyflie flies over a low texture area, wobbles, flips and crashes. This won’t happen as long as you are flying of high texture areas (like a children’s play mat for instance), but the occasional situation that it is not, it should not crash like it does now. The expected behavior of the Crazyflie should be that it glides away until it flies over something with sufficient texture again (That is the behavior that you see if when you are flying manually with a"
  },
  {
    "location": "https://www.bitcraze.io/2020/08/flow-and-lps-deck-improvements-in-progress/#chunk-2",
    "title": "Flow- and LPS-deck improvements in progress | Bitcraze (Part 2)",
    "text": "controller, and you just let the controls go). So we decided to investigate this further. First we thought that it might had something to do with the rotation compensation by the gyroscopes, which is part of themeasurement modelof the flowdeck, since maybe it was overcompensating or something like that. But if you remove that parts, it starts wobbling right away,evenwith high texture areas… so that was not it for sure… Even though we still think that it causes the actual wobbling itself (compensating flow that is not detected) but we still had to dig a bit deeper into the issue. Eventually we did a couple of measurements. We let the Crazyflie fly over a low and high texture area while flying an 8 shape and log a couple of important values. These were the detected flow, the ground truth position, and a couple of quality measurements that the Pixart’s PMW3901 flow sensor provided themselves, namely the amount of features (motion.squal) and the automatic shutter time (motion.shutter). With the ground truth position we can transform that to the ground truth flow that the flowdeck is supposed to measure. With that we can see what the standard deviation of the measurement vs"
  },
  {
    "location": "https://www.bitcraze.io/2020/08/flow-and-lps-deck-improvements-in-progress/#chunk-3",
    "title": "Flow- and LPS-deck improvements in progress | Bitcraze (Part 3)",
    "text": "groundtruth flow is supposed to be, and see if we can find a relation the error’s STD and the quality values, which resulted in these couple of nice graphs: Three major improvements were added to the code based on these results: Now when the Crazyflie flies over low texture areas with the Flowdeck alone, it will not flip anymore but simply glide away! Check out thisclosed issueto know more about the exact implementation and it should be part of the next release. The previous fix of the flow deck also took care ofthis issue, which caused the Crazyflie to also flip in the LPS system if it does not detect any flow.. This happened because the Kalman filter trusted the Flow measurement much more than the UWB distance measurement in the previous firmware version, but not anymore! If the Flowdeck is out of range or can’t detect motion, the state estimation will trust the LPS system more. However, once the Flowdeck is detecting motion, it will help out with the accuracy of the positioning estimate. Moreover, now it is possible to make the Crazyflie fly in and out of the LPS system area with the Flowdeck! however, be sure that"
  },
  {
    "location": "https://www.bitcraze.io/2020/08/flow-and-lps-deck-improvements-in-progress/#chunk-4",
    "title": "Flow- and LPS-deck improvements in progress | Bitcraze (Part 4)",
    "text": "it flies using velocity commands, since there are situations where the position estimate can skip: As long as your are flying with velocity commands, like with theassist modeswith the controller in the CFclient, this should not be a problem. The previous fixes only work with the LPS methodsTDOA2andTDOA3. Unfortunately, there is still some work to be done with the Deck incompatibility with the TWR method and the Flowdeck. The deck stops working quickly after the Crazyflie is turned on and this seems to be related to the SPI bus that is shared by the LPS deck and the flowdeck. Reading the flow sensor takes some time, which blocks the TWR algorithm for a while, making it miss an event. Since the TWR algorithm relies on a continues stream of events from the DWM1000 chip, it simply stops working if it does not … or at least that is our current theory … Please check out thisissueto follow the ongoing discussion. If you have maybe an idea of what is going on, drop a comment and see if we can work together to iron out this issue once and for all!"
  },
  {
    "location": "https://www.bitcraze.io/2020/05/latest-update-on-the-ai-deck/#chunk-1",
    "title": "Latest Update on the AI deck | Bitcraze (Part 1)",
    "text": "It has been a while since we have updated you all on the AI deck. The last fullblogpostwas in October, with somesmall updateshere and there. It is not that we have not focused on it at all; on the contrary… this has been a high priority project for a while now. It is just quite a complex board with a lot of bells and whistles, which can be challenging to work with sometimes so early in development, something thatour previous interncan definitely agree on. So therefore we rather wanted to wait until we were able to make sufficient progress before we gave you an update… and so we have! Together withGreenwaves technologieswe have been trying to get theSDK of the GAP8 chipon the AI deck stable enough for an early release. The latest release of the SDK (version 3.4) has proved itself to work with relative ease on the AI deck after extensive testing. Currently it is possible to use OpenOCD for flashing and debugging, and it supports most commonly available debuggers with a jtag connector. In the upcoming weeks both of Bitcraze and Greenwaves will test and try out all examples of the SDK on the AI deck to"
  },
  {
    "location": "https://www.bitcraze.io/2020/05/latest-update-on-the-ai-deck/#chunk-2",
    "title": "Latest Update on the AI deck | Bitcraze (Part 2)",
    "text": "make sure that everything is still compatible. Also the documentation will be extended as well. As there is so much to document, it might be difficult to catch all of it. However, if you notify us and Greenwaves on anything that is missing once the AIdeck is out, that will help us out to catch the knowledge gaps. The AI deck also contains theESP-basedNINAmodule for establishing a WiFi connection. This enables the users to stream the video stream of the AI deck onto their computers, which will be quite an essential tool if they would like to generate their own image database for training the CNNs for the GAP8 (and it happens to also be quite practical for debugging by the way!). Currently it is required to set credentials of your local WiFi network and reflash the AI-deck to be able to connect and streaming the images, but we are working on turning the Nina into an access-point instead so no reflashing would be required. We hope that we will be able to implement this before the release. We are also trying out to adjust applications to make suitable of the AI deck. For instance, we have adapted Greenwaves’ face-detector"
  },
  {
    "location": "https://www.bitcraze.io/2020/05/latest-update-on-the-ai-deck/#chunk-3",
    "title": "Latest Update on the AI deck | Bitcraze (Part 3)",
    "text": "example to use the image streamer instead of the display available on theGAPuino boards. You can see avideoof the result here underneath. Beware that this face-detector is not based on a CNN but on HOG descriptors, so it only works in good conditions where the face is well lit. However, it is possible to train a CNN to detect faces in Tensorflow and flash this on the AI deck with the GAPflow framework as developed by Greenwaves. At Bitcraze we haven’t managed to try that out ourselves ( we are close to that though!) but at least this example is a nice demonstration of the AI deck’s abilities together with the WiFi-streamer. This example and more testing code can be found in our experimental repohere. For examples of GAPflow, please check out theexamples/NNtoolsection of theGAP8 SDK. For some reason WordPress has difficulty embedding the video that was supposed to be here, so please checkhttps://youtu.be/0sHh2V6Cq-Q Seeing how the development has been progressing, we will be comfortable to say that the AI deck could be ready for early release somewhere in the next month, so please keep an eye out on our website! We will continue to test the GAP SDK’s stability"
  },
  {
    "location": "https://www.bitcraze.io/2020/05/latest-update-on-the-ai-deck/#chunk-4",
    "title": "Latest Update on the AI deck | Bitcraze (Part 4)",
    "text": "and we are very thankful for Greenwaves Technologies with their help so far. We will also work on getting-started guides in order to get acquainted with the AI deck, supplementing the already existingdocumentation about the GAP8 chip. Even-though the AI deck will soon be ready for early release, this piece of hardware is not for the faint-hearted and embedded programming experience is amust. But keep in mind that the possibilities with the AI deck are huge, as it will be mean that super-edge-computing on a 30 gram flying platform will be available for anyone. It will all be worth it when you have your Crazyflie flying autonomously while being able to recognize its surroundings :)"
  },
  {
    "location": "https://www.bitcraze.io/2019/10/enabling-swarm-exploration/#chunk-1",
    "title": "Enabling Swarm Exploration | Bitcraze (Part 1)",
    "text": "For the last four years of doing my PhD at theTU Delftand theMAVlab, we were determined to figure out how to make a swarm/group of tiny quadcopters fly through and explore an unknown indoor environment. This was not easy, as many of the sub-challenges that needed to be solved first. However, we are happy to say that we were able to show a proof-of-concept in the latestScience Roboticsissue! Here you can seethe press releasefrom the TU Delft for general information about the project. Since we used the Crazyflie 2.0 to achieve this result, this blog-post we wanted to mostly highlight the technical side of the research, of the achievements and the challenges we had to face. Moreover, we will also explain the updated code which uses the new features of the Crazyflie Firmware as explained in the previousblogpost. In the paper, we presented a technique called Swarm Gradient Bug Algorithm (SGBA), which borrows (as the name suggests) navigational elements from the path planning technique called ‘Bug Algorithms’ (see thispaperfor an overview). The basic principle is that SGBA is a state-machine with several simple behavior presets such as ‘going to the goal’, ‘wall-following’ and ‘avoiding other Crazyflies’. Here in the bottom"
  },
  {
    "location": "https://www.bitcraze.io/2019/10/enabling-swarm-exploration/#chunk-2",
    "title": "Enabling Swarm Exploration | Bitcraze (Part 2)",
    "text": "you can see all the modules were used. For the main experiments (on the left), the Crazyflie 2.0’s were equipped with theMultirangerand theFlowdeck(here we used the Flow deck v1). On the right you see the Crazyflies used for the application experiment, were we made an custom Multiranger deck (with fourVL53L0x‘s) and aHubsan Camera module. For both we used theTurnigy nanotech 300 mAh (1S 45-90C) LiPo battery, to increase the flight time to 7.5 min. With this, we were able to have 6 Crazyflies explore an empty office floor in the faculty building of Aerospace engineering. They started out in the middle of the test environment and flew all in different preferred directions which they upheld by their internal estimated yaw angle. With the multi-rangers, they managed to detect walls in their, and followed its border until the way was clear again to follow their preferred direction. Based on their local odometry measurements with the flowdeck, the Crazyflies detected if they were flying in a loop, in order to get out of rooms or other situations. A little before half way of their battery life, they would try to get back to their initial position, which they did by measuring the"
  },
  {
    "location": "https://www.bitcraze.io/2019/10/enabling-swarm-exploration/#chunk-3",
    "title": "Enabling Swarm Exploration | Bitcraze (Part 3)",
    "text": "Received Signal Strength Intensity of theCrazyradio PAhome beacon, which was located at their initial starting position. During wall-following, they measured the gradient of the RSSI, to determine in which directions it increases or decreases, to estimate the angle back the goal. While they were navigating, they were also communicating with each-other by means of broadcasting messages. Based on those measurements of RSSI, they could sense other Crazyflies approaching, which they first of all used for collision avoidance (by letting the low priority CFs move out of the way of the high priority CFs). Second of all, during the initial exploration phase, they communicated their preferred direction as well, so that one of them can change its exploration behavior to not conflict with the other. This way, we tried to maximize the explored area by the Crazyflies. One of those experiments with 6 Crazyflies can be seen in this video for better understanding: We also showed an application experiment where 4 crazyflies with the camera modules searched for 2 dummies in the same environment. In order to get the results presented above, there were many challenges to overcome during the development phase. Here is a list that explains a couple of"
  },
  {
    "location": "https://www.bitcraze.io/2019/10/enabling-swarm-exploration/#chunk-4",
    "title": "Enabling Swarm Exploration | Bitcraze (Part 4)",
    "text": "the elements that needed to work flawlessly: Many of the issues, especially the communication challenges, will be solved with the updated code implementation as explained in the next section. The firmware that the Crazyflies used to fly in the experiments showed in the paper, can all be found inthis public repository. However, the code is based quite an old version the current Crazyflie firmware, as it was forked almost a year ago. The implementation of the SGBA state machine and the P2P broadcasting were not generic enough to integrate this back to the development cycle, therefore the current code is only suitable for the old Crazyflie 2.0. Therefore, we developedtwo major changesin the latest firmware which will make it much easier for me (and other ideas as well we hope!) to implement SGBA and the P2P communication in a way that should be compatible with any version of the firmware (and hardware) from here and on. We implemented SGBA as an app-layer and also handled all the broadcast messaging directly from this layer as well. Please check out thisGithub repositorywith this new app layer implementation of SGBA."
  },
  {
    "location": "https://www.bitcraze.io/2021/04/transitioning-back-to-windows-development/#chunk-1",
    "title": "Transitioning back to Windows Development | Bitcraze (Part 1)",
    "text": "When I was started my Robotics Master back in 2012, I remembered how frustrated I was at the time to setup my development environment in Windows for the C++ beginners course. My memory is a bit fuzzy of course but I remembered it took me days to get all the right drivers, g++ libraries right, and to setup all in the path environmental in Eclipse at the time. Once I started working on Ubuntu for my Master thesis, forced to due to ROS, I was hooked and swore I will never go back to Windows for robotics development again… until now! I always used Windows on my personal machine on the side for gaming and have a dual-boot on the work computer for some occasional video editing, but especially I had begun to learn game development for Fun Fridays, I started to be drawn to the windows side of the dual boot more and more. But if I needed to try something out on the Crazyflie or needed to debug a problem on theforum, I needed to restart the computer to switch operating systems and that was starting to become a pain! Slowly but steadily I tried out several aspects"
  },
  {
    "location": "https://www.bitcraze.io/2021/04/transitioning-back-to-windows-development/#chunk-2",
    "title": "Transitioning back to Windows Development | Bitcraze (Part 2)",
    "text": "of thecrazyflie ecosystemfor development on Windows 10 and actually…. it is not so traumatic as it was almost 10 years ago. It went quite smooth when I first try toinstall python on windowsagain. Adding it to the PATH environment variable is still very important but luckily the new install manager provides that as an option. Moreover,Visual Studio Codealso provides thepossibility to switch between python environmentsso that you try different versions of python, but for now version 3.8 was plenty for me. With the newest versions of the Windows install of Python, pip is by definition already installed, but I experienced that it would still be necessary to upgrade pip by typing the following in either a Command Prompt or (my favorite) Powershell: After this, install thecflibfrom release was quite an ease (‘pip install cflib’) but eveninstalling it from sourcewithGit configured on Windowswas no problem at all and very similar to a native Ubuntu install.Until recently thecfclientwas a bit more challenging to install through pip from due the SDL2 windows library had to be downloaded separately, so the only options would have beeninstalling from sourceorthe .exe application release. The later has not been updated since the 2020.09 release due to building"
  },
  {
    "location": "https://www.bitcraze.io/2021/04/transitioning-back-to-windows-development/#chunk-3",
    "title": "Transitioning back to Windows Development | Bitcraze (Part 3)",
    "text": "errors. Luckily, withthe latest release, this has now been fixed as a SLD2 python library was found. Now the cfclient can be installed with a simple ‘pip install cfclient’. The firmware development was the next thing that I tried to get up and running, which managed to be slightly trickier. About a year ago I tried to get Cygwin to work on Windows, but my bad memories of the past came back due to the clunkiness of it all and I abandoned it again. Also there are somereported issues with the out-of-tree build(aka the App layer). Some colleagues at Bitcraze already mentioned theWindows Subsystem for Linux(WSL) but I never really looked at it until the need came to move back to Windows for development. And I must say, I wish I had tried it out a while ago. With some repositories downloaded already on my Windows system with Git, I installed Ubuntu 20.04 WSL,got the appropriate gcc librariesand accessed the crazyflie-firmware by‘cd /mnt/c/my/repos‘. Building the firmware with‘make all’went pretty okay… although it took about a minute which is a little long compared to the 20 secs on the native Ubuntu install. The big problem was that I could not useDockerandthe"
  },
  {
    "location": "https://www.bitcraze.io/2021/04/transitioning-back-to-windows-development/#chunk-4",
    "title": "Transitioning back to Windows Development | Bitcraze (Part 4)",
    "text": "handy bitcraze toolbeltdue to the WSL version still being 1. These functionalities were only available for version 2 soI went ahead and upgraded the WSLandlinked it to docker desktop. But after upgrading, building the firmware from that same repository on the C:/ drive took insanely long (almost 10 minutes). So I switched back the WSL ubuntu 20.04 to version 1, installed a second WSL (this time Ubuntu 18.04), updated that one to WSL2 and used solely for docker and toolbelt purposes. Not ideal quite yet… but luckily withvisual studio code it is very easy to switch the WSL. But there is more though! Recently I timed how long it took to build thecrazyflie-firmwarewith ‘make all -j8’ from both WSL version in a repository that is on the C:/ drive on Windows (accessible by /mnt/c from the WLS), or from a repository on the local file system: This is done on an Windows laptop with an i7-6700HQ with 32 Gb RAM. The differences with WLS2 between build firmware on the windows file system or the local WSL file system is huge! So that means that the right way is to have WSL2 with the repo on the WSL file system, which"
  },
  {
    "location": "https://www.bitcraze.io/2021/04/transitioning-back-to-windows-development/#chunk-5",
    "title": "Transitioning back to Windows Development | Bitcraze (Part 5)",
    "text": "is similar to build time as a native install of Ubuntu. The main issue still with WSL is that it does not allow USB access… So even if thecrazyradio driver is installed on windows with Zadig, you will not be able to see if you type ‘lsusb‘ in WSL for both version 1 and 2. So when I still had the repository on the C:/ drive and build the crazyflie-firmware from there I could flash the Crazyflie through the Cfclient or Cflib (with cfloader) through Powershell, but building it from the local subsystem, which is way faster for WSL2, would require to first copy the cf2.bin file to my C drive before doing that. Another option, although still in Alpha phase, is to use theexperimental Crazyradio serverfor WSL made by Arnaud, for which the user instructionscan be found in an issue thread only for now. The important thing is that theZadig installed driverhas to be switched to WinUSB and switched back again to LibUSB if you want to use the Cfclient on windows. It would still needs some work to improve the user experience but gives promise of better integration of WSL development for the Crazyflie. Soon I’m planning to"
  },
  {
    "location": "https://www.bitcraze.io/2021/04/transitioning-back-to-windows-development/#chunk-6",
    "title": "Transitioning back to Windows Development | Bitcraze (Part 6)",
    "text": "soon reinstall the Windows part on the dual boot laptop but there are already some things that I will integrate on my freshly installed Windows based on what I experienced so far: For anyAIdeck development, I would still need to use the native Ubuntu part or thebitcraze-VMsince there is not a USB access or server yet for the programmer. However, if Windows would support USB devices and a graphical interface for WSL, that will make all our Windows-based Crazyflie development dreams come true!"
  },
  {
    "location": "https://www.bitcraze.io/2024/04/python-bindings-for-the-crazyflie-firmware/ #chunk-1",
    "title": "Python bindings for the Crazyflie Firmware | Bitcraze (Part 1)",
    "text": "Today, we’d like to take the opportunity to spotlight a feature that’s been in our code base for some time, yet hasn’t been the subject of a blog post: the Python bindings for our Crazyflie firmware. You may have noticed it mentioned in previous blog posts, and now we’ll delve into more detail about what it is, how we and others are utilizing it, and what its future holds. Language bindings, in essence, are libraries that encapsulate chunks of code, enabling one programming language to interface with another. For instance, consider the projectZenoh. Its core library is crafted in Rust, but it offers bindings/wrappings for numerous other languages like Python, C/C++, and so on. This allows Zenoh’s API to be utilized in scripts or executables written in those languages. This approach significantly broadens the functionality without necessitating the rewriting of code across multiple programs. A case in point from the realm of robotics is ROS(1), which initially created all of their APIs for different languages from scratch—a maintenance nightmare. To address this, for ROS 2, they developed the primary functionality entirely in C and provided wrappers for all other programming languages. This strategy eliminates the need to ‘reinvent the wheel’"
  },
  {
    "location": "https://www.bitcraze.io/2024/04/python-bindings-for-the-crazyflie-firmware/ #chunk-2",
    "title": "Python bindings for the Crazyflie Firmware | Bitcraze (Part 2)",
    "text": "with each iteration. Rather than redeveloping the firmware in Python, our esteemed collaborators Wolfgang Hönig and James Preiss took a pragmatic approach. They selected parts of the Crazyflie firmware and wrapped them for Python use. You can see the process in thisticket. This was a crucial step for the simulation of theoriginal Crazyswarm (ROS1) projectand was continued for its use in theCrazyswarm2 project, which is based on ROS 2. They opted forSWIG, a tool specifically designed to wrap C or C++ programs for use with higher-level target languages. This includes not only Python, but also C#, GO, Javascript, and more, making it the clear choice for implementing those bindings at the time. We also strongly recommend checking out aprevious blogpost by Simon D. Levy, who used Haskell to wrap the C-based Crazyflie Firmware for C++. As previously mentioned, the Crazyswarm1 & 2 projects heavily utilize Python bindings for testing key components of the firmware (such as the high-level commander, planner, and controller) and for a (hybrid) software-in-the-loop simulation. During theproject’s installation, these Python bindings must be compiled so they can beused during simulation. This approach allows users to first test their trajectories in a simulated environment before deploying them on"
  },
  {
    "location": "https://www.bitcraze.io/2024/04/python-bindings-for-the-crazyflie-firmware/ #chunk-3",
    "title": "Python bindings for the Crazyflie Firmware | Bitcraze (Part 3)",
    "text": "actual Crazyflies. The advantage is that minimal or no modifications are required to achieve the same results. While simulations do not perfectly mirror real-world conditions, they are beneficial because they operate with the same controller as the one used on the Crazyflie itself. In our own Crazyflie simulation in Webots, it’s also possible to use these same bindings in the simulator by followingthese instructions. Three controllers (PID, Mellinger, and Brescianini), intra-drone collision avoidance, and the high-level commander planner have all been converted into Python bindings. Recently, we’ve added a new component: the Extended Kalman Filter (EKF). This addition is ideal as it allows us to test the filter with recorded data from a real Crazyflie and experiment with different measurement models. As we discussed ina previous blogpost, estimators are complex due to their dependence on chance and environmental factors. It’s beneficial for developers to have more control over the inputs and expected outputs. However, the EKF is deeply integrated into the interconnected processes within the Crazyflie Firmware. After a significantrefactoring effort, these were added to the bindings by creating an EKF emulator (seethis PR). This enabled Kristoffer to further enhancethe TDOA outlier filterfor the Crazyflie by emulating the full process"
  },
  {
    "location": "https://www.bitcraze.io/2024/04/python-bindings-for-the-crazyflie-firmware/ #chunk-4",
    "title": "Python bindings for the Crazyflie Firmware | Bitcraze (Part 4)",
    "text": "of the EKF, including IMU data. In addition to SITL simulation and EKF development, Python bindings are also invaluable for continuous integration. They enable comprehensive testing that encompasses not just isolated code snippets, but entire processes. For instance, if there’s a recording of a Crazyflie flight complete with sensor data (such as flow, height, and IMU data), and it’s supplemented with a recorded ground truth (from lighthouse/mocap), this sensor data can be fed into the EKF Python binding. We can then compare the outputted pose with the ground truth to verify accuracy. The same principle applies to the controllers. Consequently, if any changes are made to the firmware that affect these crucial aspects of Crazyflie flight, these tests can readily detect them. If you like to try the python bindings tests for yourself,clone the Crazyflie-firmware repoand build/install the python bindings viathese instructions. Make sure you are in the root of the repository and run:python3 -m pytest test_python/. Mind that you might need to put the bindings in the same path withexport PYTHONPATH=<PATH_TO_>/crazyflie-firmware/build:$PYTHONPATH(please seethis open ticket) We’ve seen how Python bindings have proven to be extremely useful, and we’re keen to further expand their application. At present, only theLoco positioningsystem has"
  },
  {
    "location": "https://www.bitcraze.io/2024/04/python-bindings-for-the-crazyflie-firmware/ #chunk-5",
    "title": "Python bindings for the Crazyflie Firmware | Bitcraze (Part 5)",
    "text": "been incorporated into the EKF part of the Python bindings. Work is now underway to enable this forthe Lighthouse system(see thisdraft PR). Incorporating the Lighthouse system will be somewhat more complex, but fortunately, much of the groundwork has already been laid, so we hope it won’t be too challenging. However, we have encountered issues when using the controller bindings with simulation (see thisopen ticket). It appears that some hardware-specific timing has been hardcoded throughout the PID controller in particular. Therefore, work needs to be done to separate the hardware abstraction from the code, necessitating additional refactoring work for the controller. Recent projects likeSim_CF2(seethis blogpost) andCrazysim(see thisdiscussion thread) have successfully compiled the Crazyflie firmware to run as a standalone node on a computer. This allows users to connect it to the Crazyflie Python library as if it were an actual Crazyflie. This full Software-In-The-Loop (SITL) functionality, already possible with autopilot suites likePX4andArdupilot, is something we at Bitcraze are eager to implement as well. However, considering the extensive work required by the aforementioned SITL projects to truly separate the hardware abstraction layer from the codebase, we anticipate that refactoring the entire firmware will be a substantial task. We’re excited to see what"
  },
  {
    "location": "https://www.bitcraze.io/2024/04/python-bindings-for-the-crazyflie-firmware/ #chunk-6",
    "title": "Python bindings for the Crazyflie Firmware | Bitcraze (Part 6)",
    "text": "we can achieve in this area. Indeed, even with a more comprehensive Software-In-The-Loop (SITL) solution, there’s no reason to completely abandon Python bindings. For developments requiring more input/output control—such as the creation of a new controller or an addition to the Extended Kalman Filter (EKF)—it’s beneficial to start with just that portion of the firmware code. Python bindings and a SITL build can coexist, each offering its own advantages and disadvantages for different stages of the development process. By leveraging the tools at our disposal, we can minimize the risk of damaging Crazyflies during development. Let’s continue to make the most of these valuable resources!"
  },
  {
    "location": "https://www.bitcraze.io/2024/01/explaining-kalman-filters-with-the-crazyflie/  #chunk-1",
    "title": "Explaining Kalman Filters with the Crazyflie | Bitcraze (Part 1)",
    "text": "A while ago, we wrote ageneric blog postabout state estimation in the Crazyflie, mostly discussing different ways the Crazyflie can determine its attitude and/or position. At that time, we only had theComplementary filterandExtended Kalman filter(EKF). Over the years, we’ve made some great additions like theM-estimation-based robust Kalman filter(an enhancement of the EKF, seethis blog post) andthe Unscented Kalman filter. However, we have noticed that some of our beginning users struggle with understanding the concept of Kalman filtering, depending on whether this has been covered in their curriculum. And for some more experienced users, it might be nice to have a recap of the basics as well, since this is a very important part of the Crazyflie’s capabilities of flight (and also for robotics in general). So, in this blog post, we will explain the principles of Kalman filtering and how it is applied within the Crazyflie firmware, which hopefully will provide a good base for anyone starting to delve into state estimation within the Crazyflie. We will also have adeveloper meeting about Kalman filtering on the Crazyflie, so we hope you can join that as well if you have any questions about how it all works. Also we are planningto"
  },
  {
    "location": "https://www.bitcraze.io/2024/01/explaining-kalman-filters-with-the-crazyflie/  #chunk-2",
    "title": "Explaining Kalman Filters with the Crazyflie | Bitcraze (Part 2)",
    "text": "got to FOSdemthis weekend so we hope to see you there too. Anybody remotely working with autonomous systems must, at one point, have heard of the Kalman filter, as it has existed since the 60s and even played a role in the Apollo program. Understanding its main principles is also important for anyone working with drones or robotics. There are plenty of resources available, and itsWikipedia pageis filled with examples, so here we will focus mostly on the concept and principles and leave the bulk of the mathematics as an exercise for those who like to delve into that :). So basically, there are several principles that apply to a Kalman filter: So the state estimate is usually a vector of different variables that the developer or user of the system likes to observe, for either control or prediction, something like position and velocity, for instance:[x, y,ẋ, ẏ, …]. One can describe a dynamics model that can predict the state in the next step using only the current time step’s state, like for instance:xt+1= xt+ẋt, yt+1= yt+ ẏt. This can also be nicely described in matrix form as well if you like linear algebra. To this model, you can also"
  },
  {
    "location": "https://www.bitcraze.io/2024/01/explaining-kalman-filters-with-the-crazyflie/  #chunk-3",
    "title": "Explaining Kalman Filters with the Crazyflie | Bitcraze (Part 3)",
    "text": "add predicted noise to make it more realistic, or the effect of the input commands to the system (like voltage to motors). We will not go into the latter in this blogpost. So, we will go through the process of explaining the steps of the Kalman filter now, which hopefully will be clear with the above picture. As mentioned before, we’d like to avoid formulas and are oversimplifying some parts to make it as clear as possible (hopefully…). First, there is thepredict phase, where thecurrent state(estimate) and adynamics model(also known as the state transition model) result in a predicted state. Also in the same phase, thepredicted estimated covarianceis calculated, which also uses the dynamics model plus an indication of theprocess noise model, indicating how much the dynamics model deviates from reality in predicting that state. In an ideal world and with an ideal model, this could be enough; however, no dynamics model is perfect, which is why the next phase is also very important. Then it’s theupdate phase, where the filter estimate gets updated by ameasurementof the real world through sensors. The measurement needs to go through ameasurement model, which transforms the measurement into ameasured state(also known as innovation or"
  },
  {
    "location": "https://www.bitcraze.io/2024/01/explaining-kalman-filters-with-the-crazyflie/  #chunk-4",
    "title": "Explaining Kalman Filters with the Crazyflie | Bitcraze (Part 4)",
    "text": "measurement pre-fit residual). Usually, a measurement is not a 1-1 depiction of one variable of the state, so the measurement model ensures that the measurement can properly be compared to the predicted state. This same measurement model, accompanied by the measurement noise model (which indicates how much the measurement differs from the real world), together with the predicted covariance, is used to calculate theinnovation and Kalman gain. The last part of the update phase is where the predictions are updated with the innovation. The Kalman gain is then used to update the predicted state to anew estimated statewith the measured state. The same Kalman gain is also used to update the covariance, which can be used for the next time step. It’s always good to show the filter in some form of example, so let’s show you a simple one in terms of height estimation to demonstrate its implications. You see here a Crazyflie flying, and currently it has its height estimated atztand its velocity atżt. It goes to the predict phase and predicts the next height to be atzt+1,predict, which is a simple model of justzt+ żt. Then for the innovation and updating phase, a measurement (from a range"
  },
  {
    "location": "https://www.bitcraze.io/2024/01/explaining-kalman-filters-with-the-crazyflie/  #chunk-5",
    "title": "Explaining Kalman Filters with the Crazyflie | Bitcraze (Part 5)",
    "text": "sensor)rzis used for the filter, which is translated tozt+1, meas. In this case, the measurement model is very simple when flying over a flat surface, as it probably is only a translation addition of the sensor to the middle of the Crazyflie, or perhaps a compensation for a roll or pitch rotation. In the background, the covariances are updated and the Kalman gain is calculated, and based onzt+1,predictandzt+1, meas, the next statezt+1is calculated. As you probably noticed, there was a discrepancy between the predicted height and measured height, which could be due to the fact that the dynamics model couldn’t correctly predict the height. Perhaps a PID gain was higher than expected or the Crazyflie had upgraded motors that made it climb faster on takeoff. As you can see here, the filter put the estimated height closer tozt+1to the measurement than the predicted height. The measurement noise model incorporated into the covariances indicates that the height sensor is more accurate than the height coming from the dynamics model. This would very well be the case for an infrared height sensor like the one on the Flow Deck; however, if it were an ultrasound-based sensor or barometer instead (which are much"
  },
  {
    "location": "https://www.bitcraze.io/2024/01/explaining-kalman-filters-with-the-crazyflie/  #chunk-6",
    "title": "Explaining Kalman Filters with the Crazyflie | Bitcraze (Part 6)",
    "text": "noisier), then the predicted height would be closer to the one predicted by the dynamics model. Also, it’s good to note that the dynamics model does not currently include the motor input, but it could have done so as well. In that case, it would have been better able to predict the jump it missed now. Let’s take it up a notch and add an extra dimension. You see here now that there is a 2D solution of the Crazyflie moving horizontally. It is at positionxt, ytand has a velocity ofẋt, ẏtat that moment in time. The dynamics model estimates the Crazyflie to end up in the general direction of the velocity factor, so it is a simple addition of the current position and velocity vector. If the Crazyflie has a flow sensor (like on the Flow Deck), flowfx, fycan be detected and translated by the measurement model to a measured velocity (part of the state filter) by combining it with a height measurement and camera characteristics. However, the measurement in the form of the measured flowfx, fyestimates that there is much more flow detected in the x-direction than in the y-direction. This can be due to a sudden wind"
  },
  {
    "location": "https://www.bitcraze.io/2024/01/explaining-kalman-filters-with-the-crazyflie/  #chunk-7",
    "title": "Explaining Kalman Filters with the Crazyflie | Bitcraze (Part 7)",
    "text": "gust in the y-direction, which the dynamics model couldn’t accurately predict, or the fact that there weren’t as many features on the surface in the y-direction, making it more difficult for the flow sensor to measure the flow in that direction. Since this is not something that both models can account for, the filter will, based on the Kalman gain and covariances, put the estimate somewhere in between. However, this is of course dependent on the estimated covariances of both the outcome of the measurement and dynamic models. It would be much simpler if the world’s processes could be described with linear systems and have Gaussian distributions. However, the world is complex, so that is rarely the case. We can make parts of the world more abstract in simulation, and Kalman filters can handle that, but when dealing with real flying vehicles, such as the Crazyflie, which is considered a highly nonlinear system, it needs to be described by a nonlinear dynamics model. Additionally, the measurements of sensors in more complex and 3D situations usually don’t have a one-to-one linear relationship with the variables in the state. Can you still use the Kalman filter then, considering the earlier mentioned principles?"
  },
  {
    "location": "https://www.bitcraze.io/2024/01/explaining-kalman-filters-with-the-crazyflie/  #chunk-8",
    "title": "Explaining Kalman Filters with the Crazyflie | Bitcraze (Part 8)",
    "text": "Luckily certain assumptions can be made that can still make Kalman filters useful in the sense of non-linearity. However, there is also the case of non-Gaussian processes in both dynamics and measurements, and in that case a complementary filter or particle filter would be best suited. The Crazyflie contains a complementary filter (which does not estimate x and y), an extended Kalman filter and an experimental unscented Kalman filter. Check out thestate-estimation documentation for more information. This is all fine and dandy, however… where can you find all of this in the code of the Crazyflie firmware? Here is an overview of where you can find it exactly in the sense of the most used filter of them all, namely the Extended Kalman Filter. There are several assumptions made and adjustments made to the regular EKF implementation to make it suitable for flight on the Crazyflie. For those details I’d like to refer to the papers on where this implementation is based on, which can be found in theEKF documentation. Also for a more precise explanation of Kalman filter, please check out the lecture slides ofStanford University on Linear dynamical systemsor theLinköping university’s course slides on Sensor Fusion. Update: From"
  },
  {
    "location": "https://www.bitcraze.io/2024/01/explaining-kalman-filters-with-the-crazyflie/  #chunk-9",
    "title": "Explaining Kalman Filters with the Crazyflie | Bitcraze (Part 9)",
    "text": "the comments we also got notified of annice EKF tutorial where you write the filter from scratch (github) from Prof. Simon D. Levy fromWashington and Lee university. Practice makes perfect! As you would have guessed, our next developer meeting will be about the Kalman filters in the Crazyflie. Keep an eye on thisDiscussion thread for more details on the meeting. Also Kimberly and Arnaud will be attending FOSdem this weekend in Brussels, Belgium. We are hoping to organize an open-source robotics BOF/meetup there, soplease let us knowif you are planning to go as well!"
  },
  {
    "location": "https://www.bitcraze.io/2024/01/the-commander-framework-part-2-offboard-or-onboard/  #chunk-1",
    "title": "The Commander Framework part 2: offboard or onboard? | Bitcraze (Part 1)",
    "text": "A few years ago, we wrote ablogpostabout the Commander framework, where we explained how thesetpoint structureworked, which drives thecontrollerof the Crazyflie, which is an essential part ofthe stabilization module. Basically, without these, there would not be any autonomy on the Crazyflie, let alone manual flight. In the blogpost, we already shed some light on where different setpoints can come from inthe commander framework, either from theCrazyflie python library(externally with theCrazyradio), thehigh level commander(onboard) or theApp layer(onboard). However, we notice that there is sometimes confusion regarding these different functionalities and what exactly sends which setpoints and how. These details might not be crucial when using just one Crazyflie, but become more significant when managing multiple drones. Understanding how often your computer needs to send setpoints or not becomes crucial in such scenarios. Therefore, this blog post aims to provide a clearer explanation of this aspect. Let’s start at the lower level from the computer. It is possible to sendvarious types of setpointsdirectly from a Python script using the Crazyflie Python library (cflib for short). This capability extends to tasks such as manual control: or for hover control (velocity control): You can check the automaticgenerated API documentation for more setpoint sending options."
  },
  {
    "location": "https://www.bitcraze.io/2024/01/the-commander-framework-part-2-offboard-or-onboard/  #chunk-2",
    "title": "The Commander Framework part 2: offboard or onboard? | Bitcraze (Part 2)",
    "text": "If you use these functions in a script, the principle is quite basic: the Crazyradio sends exactly 1 packet with this setpoint over the air to the Crazyflie, and it will act upon that. There are no secret threads opening in the background, and nothing magical happens on the Crazyflie either. However, the challenge here is that if your script doesn’t send an updated setpoint within a certain amount of time (default of2 seconds), a timeout will occur, and the Crazyflie will drop out of the sky. Therefore, you need to send a setpoint at regular intervals, like in a for loop, to keep the Crazyflie flying. This is something you need to take care of in the script. Example scripts in theCFlibthat are sending setpoints directly: Another way to handle the regular sending of setpoints automatically in the CFLib is through theMotion Commander class. By initializing a Motion Commander object (usually usinga context manager), a thread is started with takeoff that will continuously send (velocity) setpoints at a fixed rate. These setpoints can then be updated by the following functions, for instance, moving forward with blocking: or a giving body fixed velocity setpoint updates (that returns immediately): You can"
  },
  {
    "location": "https://www.bitcraze.io/2024/01/the-commander-framework-part-2-offboard-or-onboard/  #chunk-3",
    "title": "The Commander Framework part 2: offboard or onboard? | Bitcraze (Part 3)",
    "text": "check theMotion Commander’s API-generated documentationfor more functions that can be utilized. As there is a background thread consistently sending setpoints to the Crazyflie, no timeout will occur, and you only need to use one of these functions for the ‘behavior update’. This thread will be closed as soon as the Crazyflie lands again. Here are example scripts in theCFlibthat use the motion commander class: Prior to this, all logical and setpoint handling occurred on the PC side. Whether sending setpoints directly or using the Motion Commander class, there was a continuous stream of setpoint packets sent through the air for every movement the Crazyflie made. However, what if the Crazyflie misses one of these packets? Or how does this stream handle communication with many Crazyflies, especially in swarms where bandwidth becomes a critical factor? This challenge led the developers at theCrazyswarm project(nowCrazyswarm2) to implement more planning autonomy directly on the Crazyflie itself, in the form of the high-level commander. With the High-Level Commander, you can simply send one higher-level command to the Crazyflie, and the intermediate substeps (setpoints) are generated on the Crazyflie itself. This can be achieved with a regular takeoff: or go to a certain position in space:"
  },
  {
    "location": "https://www.bitcraze.io/2024/01/the-commander-framework-part-2-offboard-or-onboard/  #chunk-4",
    "title": "The Commander Framework part 2: offboard or onboard? | Bitcraze (Part 4)",
    "text": "This can be accomplished using either thePositionHLCommander, which can be used as a context manager similar to the Motion Commander (without the Python threading), or by directly employing the functions of the High-Level Commander. You can refer to the automated API documentation for the available functions of thePositionHLCommander classor theHigh-Level Commander class. Here are examples in the CFlib using either of these classes: Considering the various options available in the Crazyflie Python library, it’s essential to realize that these setpoint-setting choices, whether direct or through the High-Level Commander, can also be configured through theapp layeronboard the Crazyflie itself. You can find examples of these app layer configurations in theCrazyflie firmware repository. It’s important to note some discrepancies regarding the Motion Commander class, which was designed with the Flow Deck (relative positioning) in mind. Consequently, it lacks a ‘go to this position’ equivalent. For such tasks, you may need to use the lower-levelsend_position_setpoint()function of the regularCommander class(see thisticket.) The same applies to the High-Level Commander, which was primarily designed for absolute positioning systems and lacks a ‘go forward with x m/s‘ equivalent. Currently, there isn’t a possibility to achieve these functionalities at a lower level from theCrazyflie Python libraryas this functionality"
  },
  {
    "location": "https://www.bitcraze.io/2024/01/the-commander-framework-part-2-offboard-or-onboard/  #chunk-5",
    "title": "The Commander Framework part 2: offboard or onboard? | Bitcraze (Part 5)",
    "text": "needs to be implemented in theCrazyflie firmwarefirst (see thisticket). It would be beneficial to align these functionalities on both the CFlib and High-Level Commander sides at some point in the future. Hope this helps a bit to explain the commander frame work in more detail and where the real autonomy lies of the Crazyflie when you use different commander classes. If you have any questions on what the Crazyflie can do with these, we advise you to ask your questions ondiscussions.bitcraze.ioand we will try to point you in the right direction and give examples!"
  },
  {
    "location": "https://www.bitcraze.io/2023/11/go-with-the-flow-relative-positioning-with-the-flow-deck#chunk-1",
    "title": "Go with the Flow: Relative Positioning with the Flow deck | Bitcraze (Part 1)",
    "text": "The Flow deck has been around for some time already, officially released in 2017 (seethis blog post), and theFlow deck v2was released in 2018 with an improved range sensor. Compared toMoCap positioningand theLoco Positioning System(based on Ultrawideband) that were already possible before, optical flow-based positioning for the Crazyflie opened up many more possibilities. Flight was no longer confined to lab environments with set-up external systems; people could bring the Crazyflie home and do their hacking there. Moreover, doing research for exploration techniques that cannot rely on external positioning systems was possible with it as well. For example, back in my day as a PhD student, I relied heavily on the Flow deck formulti-Crazyflie autonomous exploration. This would have been very difficult without it. However, despite the numerous benefits that the Flow deck provides, there are also several limitations. These limitations may not be immediately familiar to many before purchasing a Crazyflie with a Flow deck. A while ago, we wrote a blog post aboutpositioning systems in generaland even delved into theLoco Positioning Systemin detail. In this blog post, we will explore the theory of how the Flow deck enables the Crazyflie to fly, share general tips and tricks for ensuring"
  },
  {
    "location": "https://www.bitcraze.io/2023/11/go-with-the-flow-relative-positioning-with-the-flow-deck#chunk-2",
    "title": "Go with the Flow: Relative Positioning with the Flow deck | Bitcraze (Part 2)",
    "text": "stable flight, and highlight what to avoid. Moreover, we aim to make the Flow deck the focus of next week’s Developer meeting, with the goal of improving or clarifying its performance further. I won’t delve into too much detail but will provide a generic indication of how the Flow deck works. As previously explained in the positioning system blog post, the Flow deck is a relative positioning system with onboard estimation. “Relative” means that wherever you start is the (0, 0, 0) position. The extended Kalman filter processes flow and height information to determine velocity, which is then integrated to estimate the position—essentially dead reckoning. The onboard Kalman filter manages this process, enabling the Crazyflie to use the information for stable hovering. The optical flow sensor (PMW3901) calculates pixel flow per frame (this old blog postexplains it well), and the IR range sensor (VL53L1x) measures height up to 4 meters (under ideal conditions). The Kalman filter incorporates a measurement model that describes the relationship between these two values and the velocity of the Crazyflie. More detailed information can be found in thestate estimation documentation. This capability allows the Crazyflie to hover, as explained in thegetting started tutorial. If you want"
  },
  {
    "location": "https://www.bitcraze.io/2023/11/go-with-the-flow-relative-positioning-with-the-flow-deck#chunk-3",
    "title": "Go with the Flow: Relative Positioning with the Flow deck | Bitcraze (Part 3)",
    "text": "to fly with the Crazyflie and the Flow deck, there are a couple of things to take in mind: There are certain situations that the Flow deck has some issues with: We made a video that shows these types of behaviors, starting of course with the most ideal flying conditions: Moreover, it is also important to note that you shouldn’t fly too high or yaw too often. The latter will make the Crazyflie drift, as the optical flow cannot be distinguished as being caused by the yaw movement. We believe that many of the issues people experience are primarily due to the invisibility of the positioning quality. In many of our examples, the Crazyflie will not take off if the position is stable. However, we don’t have a corresponding functionality in our CFclient, as it is more up to the user to recognize when the positioning is diverging. There is a lot of room for improvement in this regard. This is the reason why the next developer meeting will specifically focus on the Flow deck, which will be onWednesday the 6th of December, 3 pm central European time. During the meeting, we will explain more about the Flow deck, discuss"
  },
  {
    "location": "https://www.bitcraze.io/2023/11/go-with-the-flow-relative-positioning-with-the-flow-deck#chunk-4",
    "title": "Go with the Flow: Relative Positioning with the Flow deck | Bitcraze (Part 4)",
    "text": "the issues we are facing, and explore ways to enhance the visibility of positioning quality. Check outthis discussion threadfor information on how to join."
  },
  {
    "location": "https://www.bitcraze.io/2023/10/development-plans-for-crazyflie-simulation/#chunk-1",
    "title": "Development plans for Crazyflie Simulation | Bitcraze (Part 1)",
    "text": "It seems that many of you are very interested in simulation. We might have gotten the hint when we noticed that ourJuly’s development meetinghad our best attendance so far! Therefore, we will be planning anew developer meetingto discuss the upcoming plans for supporting simulation for the Crazyflie. Perhaps you are not aware, but there is actually aGetting Started tutorial for simulationthat has been available for a little over 2 months now. Unfortunately, circumstances prevented us from writing a blog post about it, but we’ve noticed that not all of you are aware of it yet! The getting-started tutorial demonstrates how to set up theWebots simulator, which already includes Crazyflie models and some cool examples: The latter is based on theexample app layer for wall-followingin the crazyflie-firmware repository. Starting this year, there’s also aPython library equivalentavailable. The tutorial concludes with instructions on how to edit these controllers. Alternatively, you can choose to run the files directly from thecrazyflie-simulation repository. After completing the tutorial, you can explore thesimulation repository documentationfor more information and to access additional examples. With so many plans and so little time! This is a common phrase at Bitcraze, and it’s a symptom of being an overly ambitious, but"
  },
  {
    "location": "https://www.bitcraze.io/2023/10/development-plans-for-crazyflie-simulation/#chunk-2",
    "title": "Development plans for Crazyflie Simulation | Bitcraze (Part 2)",
    "text": "too small, team. By the way,we are still looking for more people:). Nonetheless, we have big plans to take our Crazyflie simulation to the next level: Of course, there are probably more improvements that we haven’t thought of yet, but that’s why we have developer meetings! We will be hosting another developer meeting on November 1st at 15:00 Central European Time (accounting for the time-shift from summer to autumn). You can find details on how to join in thediscussion thread here. Just for your information, I (Kimberly) am the main driving force behind our simulation efforts. However, I’m currently on partial sick leave and will soon be on full leave for a while. I kindly ask for your patience with the pace of ongoing developments. Remember, it’s an open-source project, so if you’d like to contribute and help out, we would greatly appreciate it :)"
  },
  {
    "location": "https://www.bitcraze.io/2023/04/safety-and-the-brushless/ #chunk-1",
    "title": "Safety and the Brushless | Bitcraze (Part 1)",
    "text": "It is easy to forget that the reason why it is nice to develop for the Crazyflie is because it weighs only about 30 grams. In case something goes wrong with your script or there is a fly-away, you can simply pick it up from the air without worrying about the propellers hitting you. Moreover, when the Crazyflie crashes, it usually only requires a brush off and a potential replacement of a motor-mount or propeller. The risk of damage to yourself, other people, indoor furniture, or the vehicle itself is extremely low. However, things become very different if you’ve built a larger platform with theBoltorBQ deckwith large brushless motors (like withthis blogpost), where the risk of injury to people or to the vehicle itself increases significantly. That is one of the major reasons why the BQ deck and the Bolt are still in early access and have been for a while. In our efforts to get it out of early access, it’s time to start thinking about safety features. In this blog post, we’ll be discussing how other open-source autopilot programs are implementing safety features, followed by a discussion on current efforts for Crazyflie, along with an announcement of thedeveloper"
  },
  {
    "location": "https://www.bitcraze.io/2023/04/safety-and-the-brushless/ #chunk-2",
    "title": "Safety and the Brushless | Bitcraze (Part 2)",
    "text": "meeting scheduled for May 3rd(see below for more info). We are a bit late to the game in terms of safety compared to other autopilot programs such asPX4,ArduPilot,BetaflightandPaparazzi UAV, which have been thinking about safety for quite some time. It makes a lot of sense when you consider the types of platforms that run these autopilots, such as large fixed VTOL or fixed-wing vehicles or 10-kilo quadcopters with cinematic cameras, or the degree of outdoor flight regulation. Flying a UAV autonomously or by yourself has become much more challenging as the US, EU, and many other countries have made it more restrictive. In most cases, you are not even allowed to fly if fail-safes are not implemented, such as what to do if your vehicle loses GPS signal. These types of measures can be separated into pre-flight checks and during-flight checks. Before a vehicle is allowed to fly, or even before the motors are allowed to spin, which is called ‘arming’, several conditions must be met. First, it needs to be checked if all internal sensors, such as the IMU, barometer, and magnetometer, are calibrated and functional, so they don’t give values outside of their normal operating range. Then, the"
  },
  {
    "location": "https://www.bitcraze.io/2023/04/safety-and-the-brushless/ #chunk-3",
    "title": "Safety and the Brushless | Bitcraze (Part 3)",
    "text": "vehicle must receive a GPS signal, and the internal state estimator (usually an extended Kalman filter) should converge to a position based on that information. It should also be determined if an external remote control is connecting to the vehicle and if there is any datalink to a ground station for telemetry. Feasibility checks can also be implemented, such as ensuring that the mission loaded to the UAV is not outside its mission parameters or that the start location is not too far away from its take-off position (assuming the EKF is functional). Additionally, the battery should not be low, and the vehicle should not still be in an error state from a previous flight or crash. All of these features have the potential to be turned off or made less restrictive, depending on your situation. However, keep in mind that changing any of these may require recertification of the drone or make it fall outside what is required for outdoor flight regulation. Therefore, these should only be changed if you know what you are doing. Preflight checks documentation Now that the pre-flight checks have passed, the UAV is armed and you have given it the takeoff command. However, there"
  },
  {
    "location": "https://www.bitcraze.io/2023/04/safety-and-the-brushless/ #chunk-4",
    "title": "Safety and the Brushless | Bitcraze (Part 4)",
    "text": "is so much more that can go wrong during a UAV flight, and takeoff is one of the most dangerous moments where everything could go wrong. Therefore, there are many more safety features, aka failsafes, during the flight than for the pre-flight checks. These can also be separated into ‘triggers’ and ‘behaviors,’ so that the developer can choose what the UAV should do in case of a failure, such as ‘GPS loss’ to ‘land safely’ and so on. Thus, there are triggers that can enable the autopilot’s failsafe mechanics: Also, sometimes the support of an external Automatic Trigger system is required, which is a box that monitors the conditions where the UAV should take action in case there is no GPS, other aerial vehicles are nearby, or the UAV is crossing a geofence determined by outdoor flight restrictions. Note that all of these triggers usually have a couple of conditions attached, such as the level of the ‘low battery’ or the number of seconds of ‘GPS loss’ deemed acceptable. If any of the conditions mentioned above are triggered, most autopilot suites have some failsafe behaviors linked to those set by default. These behaviors can include the following: Usually, these actions"
  },
  {
    "location": "https://www.bitcraze.io/2023/04/safety-and-the-brushless/ #chunk-5",
    "title": "Safety and the Brushless | Bitcraze (Part 5)",
    "text": "are set in regulation, but per trigger, it is possible to give a different behavior than the default. One can decide to completely disarm the vehicle, but then the chances of the UAV crashing are pretty high, which can result in damage to the vehicle or cause harm to people or objects. By the way: disarming is the opposite act of arming, which isnotallowing the motors to spin, no matter if it is receiving an input. If you decide to never do anything and force the drone to finish the mission autonomously, then in a case of GPS or position loss, you risk losing your vehicle or that it will end up in areas where it is absolutely not allowed, such as airports. Again, changing these default behaviors should be done by someone who knows what they are doing, and it should be done with careful consideration. Failsafe documentation other autopilot suites: Fail-safes are measures that ensure safe flight. However, there will always be a chance that an emergency will occur, which will require an immediate action as well. If the vehicle has crashed during any of its phases or has flipped, or if the hardware breaks, such as the"
  },
  {
    "location": "https://www.bitcraze.io/2023/04/safety-and-the-brushless/ #chunk-6",
    "title": "Safety and the Brushless | Bitcraze (Part 6)",
    "text": "motors, arms, or perhaps even the autopilot board itself, what should be done then? The standard default behavior for this is to completely disarm the vehicle so that it won’t react to any input to the motors itself. Of course, it’s difficult to do if the autopilot program is on, but at least it won’t try to take off and finish its mission while laying on its side. It might be that a backup system is connected to the ESCs that will take over in case the autopilot is not responding anymore, perhaps using a different channel of communication. Also, the most important safety feature of all is the pilot itself. Each remote control should have a special button or switch that can put the drone in a different mode, make it land, or disarm it so that the pilot can act upon what they see. In case the motors are still spinning, have a net or towel available to throw over them, disconnect the battery as soon as possible, and make sure to have sand or a special fire retardant in case the LiPo batteries are pierced. All of the autopilots have some tips to deal with such situations,"
  },
  {
    "location": "https://www.bitcraze.io/2023/04/safety-and-the-brushless/ #chunk-7",
    "title": "Safety and the Brushless | Bitcraze (Part 7)",
    "text": "but make sure to do some good research yourself on how to handle spinning parts or potential LiPo battery fires. I’m just giving a compilation of tips given in the documentation above here, but please make sure to read up in detail! So how about the Crazyflie-firmware ? We have some safety features build in here and there but it is all over the code base. Since the Crazyflie is so safe, there was no immediate need for this and we felt it is more up to the developer to integrate it themselves. But with the Bolt and BQ deck coming out of early access, we want to at least do something. As we started already started looking into how other autopilot softwares are doing it, we can get some ideas, however we did notice that many of these are mostly meant for outdoor flight. The Crazyflie and the Crazyflie Bolt have been designed for indoor use and perhaps deal with different issues as well. This is a collection of safety features currently in the firmware at the time of writing this blogpost. Most safety features in the Crazyflie are up for the developer to double check before and during"
  },
  {
    "location": "https://www.bitcraze.io/2023/04/safety-and-the-brushless/ #chunk-8",
    "title": "Safety and the Brushless | Bitcraze (Part 8)",
    "text": "flight, but these are some automatic once that are scattered around the firmware: However, if for instance your Crazyflie or Bolt platform loses its positioning in air, or doesn’t have a flowdeck attached before takeoff, there are no default safety systems in check. You either need to catch it, make it land or use an self-madeemergency stop buttonusing one of the emergency stop services above. As mentioned earlier, we have safety features spread throughout the code base of the Crazyflie firmware. Our current effort is to collect all of these emergency stops and triggers in thesupervisor moduleto have them all in one place. In addition, since indoor positioning is critical, we want to be notified when it fails. For instance, if the lighthouse geometry is incorrect, we need to see if the position diverges. This check was done outside of the Crazyflie firmware in a cflib script, but it has not been implemented inside the firmware. We also want to provide some options in terms of behavior for these triggers. Currently, we are working on two options: ‘turn the motors off’ or ‘safe land,’ with ‘safe land’ decreasing the thrust while keeping the drone level in attitude. Furthermore, we want"
  },
  {
    "location": "https://www.bitcraze.io/2023/04/safety-and-the-brushless/ #chunk-9",
    "title": "Safety and the Brushless | Bitcraze (Part 9)",
    "text": "to integrate these features into the cfclient as well. For example, we want to add more emergency safety features to our remote control through thecfclient, and show users how to arm and disarm the vehicle. These are the elements we are currently working on, but there might be more to come! You probably already guessed it… the topic about the next developer meeting will be about the safety features in the Crazyflie and the Bolt! We will present the current safety features in the Crazyflie and what we are currently working on to make it better. In this sense, we really want to have your feedback on what you think is important for brushless versions of the Crazyflie for indoor flight! The Dev meeting will be on Wednesday May the 3rd at 3 PM CEST. Please keep an eye on the discussion forum in thedeveloper meeting thread."
  },
  {
    "location": "https://www.bitcraze.io/2021/05/positioning-system-overview/ #chunk-1",
    "title": "Positioning System Overview | Bitcraze (Part 1)",
    "text": "As you have noticed, we talk about the lighthouse positioning a lot these last couple of months ever sincewe got it out of early release.However, it is good to realize that it is not the only option out there forpositioning your Crazyflie! That is why in this blog-post we will lay out possible options and explain how they are different/similar to one another. The first we will handle are the use of motion capture systems (MoCap), which resolves around the use of InfraRed cameras and Markers. We use theQualysiscamera ourselves but there are also labs out there that useViconorOptitrack. The general idea is that the cameras have an IR-light-emitting LED ring, which are bounced back by reflective markers that are supposed to be on the Crazyflie. These markers can therefore be detected by the same cameras, which pass through the marker positions to an external computer. This computer will have a MoCap program running which will turn these detected markers into a Pose estimate, which will in turn be communicated to the Crazyflie by aCrazyradio PA. Since that the positioning is estimated byan external computerinstead of onboard of the crazyflie, a MoCap positioning system is categorized as anoff-board pose estimationusing"
  },
  {
    "location": "https://www.bitcraze.io/2021/05/positioning-system-overview/ #chunk-2",
    "title": "Positioning System Overview | Bitcraze (Part 2)",
    "text": "anabsolute positioning system. For more information, please check theMotion Capture positioning documentation. The next category is a bit different and it consists of both theLocopositioning system and theLighthousepositioning system. Even though these both use beacons/sensors that areplaced externallyof the Crazyflie, the pose estimation is doneall on-boardin the firmware of the Crazyflie. So there is no computer that is necessary to communicate the position back to the Crazyflie.Remember that you do need to communicate the reference set-points or high level commands if you are not using theApp layer. Of course there are clear differences in the measurement type. A Crazyflie with theLocodeckattached takes the distance to the externally placed nodes as measured by ultra wide band (UWB) and theLighthouse deckdetects the light plane angles emitted by theLighthouse Base Stations.However the principle is the same that those raw measurements are used as input tothe Extended Kalman filter onboard of the Crazyflie,and outputs the estimated pose after fusing with the IMU measurements. Therefore these systems can be classified asabsolute positioning systemswithon-board pose estimation. To learn more please read theLocoandLighthouse positioning system documentation! It is not necessary to have to setup an external positioning system in your room in order to achieve a form"
  },
  {
    "location": "https://www.bitcraze.io/2021/05/positioning-system-overview/ #chunk-3",
    "title": "Positioning System Overview | Bitcraze (Part 3)",
    "text": "of positioning on the Crazyflie. With theFlowdeckattached, the Crazyflie can measure flows per frame with an optical flow sensor and the height in millimetres with a time of flight sensor. These measurements are then fused together with the IMU within the Extended Kalman filter (see theFlow deck measurement model), which results in a on-board pose estimation. The most important difference here to note is that positioning estimated by only the Flowdeck, will not result in a absolute positioning estimate but arelative one. Instead of using an external placed system (like MoCap, Lighthouse and Loco) which dictate where the zero position is in XYZ, the start-up position the Crazyflie determines where the origin of thecoordinate system is.That is why the Flowdeck is classified as aRelative Positioning SystemwithOn-board Pose Estimation. Oh boy… that is a different story. Theoretically it could be possible by using theonboard accelerometers of the crazyflieand fusing those in some short of estimator, however practice has shown that the Crazyflie’s accelerometers are too noisy to result in any good pose estimation… We haven’t seen any work that has been successfully to achieve any stable hover on only the IMU of the Crazyflie, but if you have done/see research that"
  },
  {
    "location": "https://www.bitcraze.io/2021/05/positioning-system-overview/ #chunk-4",
    "title": "Positioning System Overview | Bitcraze (Part 4)",
    "text": "has, please let us know! And if you would like to give a go yourself and build an estimator that is able to do this, please check out the newout of tree build functionality for estimators. This is still work in progress so it might have some bugs, but it should enable you to plugin in your own estimator separate from the Crazyflie firmware ;) We try to keep keep all the information of all our positioning systems on our website. So check out the positioning system overview pageto be referred to more details if you would be interested in a particular system that fits your requirements!"
  },
  {
    "location": "https://www.bitcraze.io/2020/07/ai-deck-sees-in-color/ #chunk-1",
    "title": "AI-deck Sees in Color | Bitcraze (Part 1)",
    "text": "It has been about a month since theAI-deckbecameavailable in Early Access. Since then there are now quite a few of you that own an AI-deck yourself. A new development we would like to share: we thought before that we had selected a gray-scale image sensor. However, it came to our attention that the camera actually contains acolor image sensor, which on second viewing of the video presented in thisblogpostis pretty obvious in hindsight (thanksPULP project ETH Zurichfor letting us know!). This came as a little surprise, but a color camera can also add some new possibilities, like making the Crazyflie follow a orange ball, or also train the CNNs incorporate color in their classification training as well. The only thing is that it will require an extra preprocessing task in order to retrieve the color image, which will be explained in the next section. Essentially all CMOS image sensors are gray-scale by definition. In order to retrieve color from a scene, manufacturers add a Bayer filter on top of the image sensor, so it filters out the red, green and blue on each pixels. ThisColor filter arraydoes not need to be RGB, but all kinds of colors, but we will"
  },
  {
    "location": "https://www.bitcraze.io/2020/07/ai-deck-sees-in-color/ #chunk-2",
    "title": "AI-deck Sees in Color | Bitcraze (Part 2)",
    "text": "only talk about the Bayer filter. If the pattern of the filter is known, the pixels that related to a certain color will be interpolated with each-other in order to fill in the gaps in between. This process is calleddemosaicingand it creates the RGB channels that are converted to a color image. Currently we only implemented a simple nearest-neighbor interpolation scheme for demosaicing, which is fine for demonstration purposes, however is not the best technique out there. Such a simple interpolation is not very ‘edge and detail’ aware and can therefore cause artifacts, like these Moiré effects seen here below. Anyway, we are still experimenting how to get a better image and how to translate that to all the examples ofthe AI-deck example repository(see thisissueif you would like to follow or take part in the discussion). So technically, once we have the color image, this can be converted to a gray-scale images which can be used for the examples as is. However, there is a reduction in quality since the full pixel resolution is not used for obtaining the full scale image. We are currently discussing if it would be useful to get the gray-scale version of this camera and"
  },
  {
    "location": "https://www.bitcraze.io/2020/07/ai-deck-sees-in-color/ #chunk-3",
    "title": "AI-deck Sees in Color | Bitcraze (Part 3)",
    "text": "make this available as well,so let us know if you would be interested! Like we said before, there now quite a few of you out there that have an AI-deck in their procession. As it is in Early Access, the software part is still in full development. However, since we have not received any negative feedback of you, we believe that everything is fine and peachy! Just kidding ;) we know that the AI-deck is quite a challenging deck to work with and we know for sure that many of you probably have questions or have something to say about working with it. Buying anEarly Accessproduct also comes with a little bit of responsibility. The more feedback we get from you guys, the more we can tailor the software and support to help you and others, thereby advancing the product forward and getting it out of the early access phase. So please, let us know if you are having any trouble starting up by posting a thread on theforum(we have a specialAI-deck group!). If there are any issues with the examples or the documentation of theAI-deck repo. We and also our collaborators atGreenwaves Technologies(from the GAP8 chip) are more than"
  },
  {
    "location": "https://www.bitcraze.io/2020/07/ai-deck-sees-in-color/ #chunk-4",
    "title": "AI-deck Sees in Color | Bitcraze (Part 4)",
    "text": "happy to help out. That is what we are here for :)"
  },
  {
    "location": "https://www.bitcraze.io/2020/05/the-commander-framework/ #chunk-1",
    "title": "The Commander Framework | Bitcraze (Part 1)",
    "text": "Here is another blog post where we try to explain parts of the stabilizer framework of the Crazyflie. Last time, we talked about thecontrollersandstate estimatorsas part of the stabilizer.cmodule which was introduced in thisblog postback in 2016. Today we will go into the commander framework, which handles the setpoint of the desired states, which the controllers will try to steer the estimated state to. The commander module handles the incoming setpoints from several sources (src/modules/src/commander.c in thefirmware). A setpoint can be set directly, either through a python script using thecflib/cfclientorthe app layer(blue pathways in the figure), or by the high-level commander module (purple pathway). The High-level commander in turn, can be controlled remotely from the python library or from inside the Crazyflie. It is important to realize that the commander module also checks how long ago a setpoint has been received. If it has been a little while (defined by threshold COMMANDER_WDT_TIMEOUT_STABILIZE in commander.c), it will set the attitude angles to 0 on order to keep the Crazyflie stabilized. If this takes longer than COMMANDER_WDT_TIMEOUT_SHUTDOWN, a null setpoint will be given which will result in the Crazyflie shutting down its motors and fall from the sky. This won’t happen if"
  },
  {
    "location": "https://www.bitcraze.io/2020/05/the-commander-framework/ #chunk-2",
    "title": "The Commander Framework | Bitcraze (Part 2)",
    "text": "you are using the high level commander. In order to understand the commander module, you must be able to comprehend the setpoint structure. The specific implementation can be found in src/modules/interface/stabilizer_types.h as setpoint_t in the Crazyfliefirmware. There are 2 levels to control, which is: These can be controlled in different modes, namely: So if absolute position control is desired (go to point (1,0,1) in x,y,z), the controller will obey values givensetpoint.position.xyzifsetpoint.mode.xyzis set tomodeAbs. If you rather want to control velocity (go 0.5 m/s in the x-direction), the controller will listen to the values given insetpoint.velocity.xyzif setpoint.mode.xyzis set tomodeVel. All the attitude setpoint modes will be set then to disabled (modeDisabled). If only the attitude should be controlled, then all the position modes are set tomodeDisabled. This happens for instance when you are controlling the crazyflie with a controller through the cfclient in attitude mode. As already explained before: The high level commander handles the setpoints from within the firmware based on a predefined trajectory. This was merged as part of theCrazyswarmproject of theUSC ACT lab(see thisblogpost). The high-level commander uses a planner to generate smooth trajectories based on actions like ‘take off’, ‘go to’ or ‘land’ with 7th order polynomials."
  },
  {
    "location": "https://www.bitcraze.io/2020/05/the-commander-framework/ #chunk-3",
    "title": "The Commander Framework | Bitcraze (Part 3)",
    "text": "The planner generates a group of setpoints, which will be handled by the High level commander and send one by one to the commander framework. It is also possible to upload your own custom trajectory to the memory of the Crazyflie, which you can try out with the script examples/autonomous_sequence_high_level of.py thecrazyflie python library repository.Please see thisblogpostto learn more. There are four main ways to interact with the commander framework from the python library. We are busy documenting the stabilizer framework in theCrazyflie firmware documentation, including the content of this blogpost. If you feel that anything is missing or not explaining clearly enough about the stabilizer framework, please drop a comment below or comment on theforum."
  },
  {
    "location": "https://www.bitcraze.io/2020/02/out-of-control/ #chunk-1",
    "title": "Out of Control | Bitcraze (Part 1)",
    "text": "Two weeks ago, we had a blogpost about thestate estimatorsthat are available within the Crazyflie. So once the Crazyflie knows where it is, it would need to be determined where it wants to go, by means of the high level commander (implemented as part of thecrazyswarm project) or set-points given byCFclientor directly from scripts usingCrazyflie python lib. But exactly how would the crazyflie get to those desired positions in the first place? The differences between the current state estimates and the desired state, will need to be transformed to inputs given to the motors. Unfortunately, quadrotors like the Crazyflie do not have easy dynamics to maintain, so if you want to learn more, see thisblogpostto read more about it! So in order use the thrust of the motors in an useful way to get the Crazyflie to do what you want to do, there are several controllers to consider, which you can see on this quick overview here underneath. It shows the different control paths that can be taken from the high level commander all the way to the power distribution of the motors. Bear in mind that these are still simple representations and that the actual implementation is of"
  },
  {
    "location": "https://www.bitcraze.io/2020/02/out-of-control/ #chunk-2",
    "title": "Out of Control | Bitcraze (Part 2)",
    "text": "course a bit more complicated, but at least it will give you a rough idea of which paths are possible to pursue. So the default settings in the Crazyflie firmware is theproportional integral derivative (PID)control for all desired state aspects. So the High Level Commander (HLC) will send desired position set-points to the PID position controller (which used to be done off-board, so outside of the Crazyflie firmware before thisblogpost). These result in desired pitch and roll angles, which are sent directly to the attitude PID controller. These determine the desired angle rates which is send to the angle rate controller (which is… you guessed… also a PID controller). This is also calledCascaded PID controller. That results in the desired thrusts for the roll pitch yaw and height that will be handled by the power distribution by the motors. (Note that height is mostly handled by the position controller) So the Incremental Nonlinear Dynamic Inversion (INDI) controller is an controller that immediately deal with the angle rates to determine the trust. This is a very new addition to the Crazyflie firmware by one of our community members and is based on the implementation of thispaper. Currently, the position control is"
  },
  {
    "location": "https://www.bitcraze.io/2020/02/out-of-control/ #chunk-3",
    "title": "Out of Control | Bitcraze (Part 3)",
    "text": "still handled by the same PID controller mentioned in the last paragraph, Nevertheless for handling the angles, it should be faster than the attitude and rate PID controller combined. We have not yet fully tested this out but if you do, let us know how you like it on the Bitcrazeforum! As part of the Crazyswarm project, the controller designed by Daniel Mellinger has been implemented in the Crazyflie firmware as well. Please see thispaperabout the details of the Mellinger controller. It is a sort of “all in one”: based on the desired position and velocity vectors towards those position, it will calculate right away what the desired thrusts are that need to be distributed to all the motors. This results in a much smoother controlled trajectory of the high level commander and therefore advised to use when the Crazyflie has a precise position estimate (lighthouse and mocap). However, as it is so aggressive, any position estimate of a lesser quality (flowdeck or LPS) will not be sufficient for this controller. See some examples of mellinger controlled flightshereandhere. So do you have experience working with these controllers or want to know more about them, please drop us a message on"
  },
  {
    "location": "https://www.bitcraze.io/2020/02/out-of-control/ #chunk-4",
    "title": "Out of Control | Bitcraze (Part 4)",
    "text": "theforum! We are currently working on stabilization and documentation of multiple aspects of the Crazyflie and the controllers is one of them, so we are really interested what your experiences are!"
  },
  {
    "location": "https://www.bitcraze.io/2020/01/state-estimation-to-be-or-not-to-be/ #chunk-1",
    "title": "State estimation: To be or not to be! | Bitcraze (Part 1)",
    "text": "How does a Crazyflie manage to fly and stay in the air in the first place? Many of us tend to take this for granted as much research tend to happen on the application level. Although we try to make the low level elements of flight as stable as possible, it might happen that whatever you are trying to implement on the application level actually effects the Crazyflie on the low level controls and estimation. We therefore would like to focus a little bit on the inner-workings of the autopilot of the Crazyflie, starting withstate estimation.The state estimation is part of the stabilizer loop in the Crazyflie, an overview of is was made in aprevious blog post. State estimation is really important in quadrotors (and robotics in general). The Crazyflie needs to first of all know in which angles it is at (roll, pitch, yaw). If it would be flying at a few degrees slanted in roll, the crazyflie would accelerate into that direction. Therefore the controller need to know an good estimate of current angles’ state and compensate for it. For a step higher in autonomy, a good position estimate becomes important too, since you would like it to"
  },
  {
    "location": "https://www.bitcraze.io/2020/01/state-estimation-to-be-or-not-to-be/ #chunk-2",
    "title": "State estimation: To be or not to be! | Bitcraze (Part 2)",
    "text": "move reliably from A to B. There are two types of state estimators in the crazyflie firmware, namely aComplementary Filterandan Extended Kalman Filter. The complementary filter is consider a very lightweight and efficient filter which in general only uses the IMU input of the gyroscope (angle rate) and the accelerator. The estimator has been extended to also include input of the ToF distance measurement of theZranger deck. The estimated output is the Crazyflie’s attitude (roll, pitch, yaw) and its altitude (in the z direction). These values can be used by the controller and are meant to be used for manual control. If you are curious how this code is implemented exactly, we encourage you to checkout the firmware inestimator_complementary.candsensfusion6.c. The complementary filter is set as thedefaultstate estimator on the Crazyflie firmware. The (extended) Kalman filter is an step up in complexity compared to the complementary filter, as it accepts more sensor inputs of both internal and external sensors. It is an recursive filter that estimates the current state of the Crazyflie based on incoming measurements (in combination with a predicted standard deviation of the noise), the measurement model and the model of the system itself. We will not go into"
  },
  {
    "location": "https://www.bitcraze.io/2020/01/state-estimation-to-be-or-not-to-be/ #chunk-3",
    "title": "State estimation: To be or not to be! | Bitcraze (Part 3)",
    "text": "detail on this but we encourage people to learn more about (extended) Kalman filters by reading up some material likethis. Shortly said, because of the more state estimation possibilities, we preferred the Kalman filter in combination with several decks:Flowdeck,Loco positioning deckand thelighthouse deck. If you look in the deck driver firmware (like for instancethis one), you see that we set the required estimator to be the Kalman and that is of course because we want position/velocity estimates :). Important though is that each input of the measurement effects the quality of the position, as positioning of the Lighthouse deck (mm precision) is much more accurate that the loco positioning deck (cm precision), which has all to do with the standard deviation of the measurement of those values. Please check out the content ofestimator_kalman.candkalman_core.c to know more about the implementation. Also good to know that the Kalman filter has ansupervisor, which resets if the position or velocity estimate is gets out of hand. Of course this blogpost does not show the full detailed explanation of state estimation, but we do hope that it gives some kind of overview so you know where to look if you would like to improve anything."
  },
  {
    "location": "https://www.bitcraze.io/2020/01/state-estimation-to-be-or-not-to-be/ #chunk-4",
    "title": "State estimation: To be or not to be! | Bitcraze (Part 4)",
    "text": "The Kalman filter can easily be extended to accept more inputs, or the models on which the estimates are based can be improved. If you would like implement your own filter, that would be perfectly possible to do so too. It would be great if you guys could share your thoughts and questions about the state estimation on the crazyflie on theforum!"
  },
  {
    "location": "https://www.bitcraze.io/2021/05/lighthouse-positioning-accuracy/ #chunk-1",
    "title": "Lighthouse Positioning Accuracy | Bitcraze (Part 1)",
    "text": "Now that theLighthouse deck is out of early accessand we have made iteasier to setup a lighthouse positioning system, we are currently at the next stage: showing how awesome it is! We feel that there are not enough people out there that know about theLighthouse positioning systemand sometimes confuse it even with theLoco position system(to be honest, the abbreviationLPSmakes it challenging). But we are confident that the Lighthouse system is a good alternative for those that want to do drone research but are on a tight budget. DuringWolfgang Hönig‘s time here at Bitcraze, one of the bigger projects we worked together on was to generate a dataset comparing the positioning quality of the Lighthouse system with a Motion Capture (MoCap) system. You could imagine that would be a difficult task, since asthe lighthouse basestationstransmit infrared light sweeps andMoCap camerasby default also emit IR light which are reflected back by markers. However, with theActive marker deckfor theQualysis system, we were able to use the MoCap and Lighthouse positioning without too much interference. Moreover, Wolfgang also helped out with improving the logging quality on theMicro-SD-card deckwhich also enabled us to get as much data real-time as possible. He wrotea blogpost about event-based"
  },
  {
    "location": "https://www.bitcraze.io/2021/05/lighthouse-positioning-accuracy/ #chunk-2",
    "title": "Lighthouse Positioning Accuracy | Bitcraze (Part 2)",
    "text": "logginga few weeks ago which is a new approach to record data on the Crazyflie at a fast pace. With theActive Marker Deck, theMicro-SD-card deckand of course theLighthouse deck,… the Crazyflie turn into a full-blown positioning data-collection machine! About this whole process, we wrote the following paper:Lighthouse Positioning System: Dataset, Accuracy, and Precision for UAV Research,A.Taffanel, B. Rousselot, J. Danielsson, K. McGuire, K. Richardsson, M. Eliasson, T. Antonsson, W. Hönig, ICRA Workshop on Robot Swarms in the Real World, Arxiv2021 This paper contains an short explanation of the lighthouse system, how we set up the data collection and an analysis of the results, where we compared both Lighthouse V1 and V2 with the Crossing beam (C.B.) method and the extended Kalman filter. In all cases, the mean and median Euclidean error of the Lighthouse positioning system are about 2-4 centimeters compared to our MoCap system as ground truth. Check outthe lighthouse dataset paperto read all the details of the experiments! Our paper is selected for a poster presentation at theICRA 2021Workshop:Robot Swarms in the Real World. So if you have any questions about the paper, please join and ask us in person! The workshop will be held on the4th of"
  },
  {
    "location": "https://www.bitcraze.io/2021/05/lighthouse-positioning-accuracy/ #chunk-3",
    "title": "Lighthouse Positioning Accuracy | Bitcraze (Part 3)",
    "text": "June. Moreover, we also are sponsoring the event by giving away aLighthouse Swarm Bundleto whomever wins the best video-demonstration award! So to all the participants, the best of luck! We are super curious to what you’ll have to show us."
  }
]